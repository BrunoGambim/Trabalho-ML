{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "#from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn import FunctionSampler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, KMeansSMOTE, SVMSMOTE, ADASYN, RandomOverSampler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2, SelectPercentile, SelectFromModel, SequentialFeatureSelector, SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "from imblearn.base import SamplerMixin\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualFeatureSelectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        print(\"fit - ManualFeatureSelectorTransformer - begin\", X.shape)\n",
    "        print(\"fit - ManualFeatureSelectorTransformer - end\", X.shape)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        print(\"transform - ManualFeatureSelectorTransformer\", X.shape)\n",
    "        #X.drop('Radiacao Global', axis=1, inplace=True)\n",
    "        X.drop('Cidade', axis=1, inplace=True)\n",
    "        X.drop('Codigo', axis=1, inplace=True)\n",
    "        print(\"transform - ManualFeatureSelectorTransformer - end\", X.shape)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImputerTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.imputer_numeric = SimpleImputer(strategy='mean')\n",
    "        self.imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print(\"fit - ImputerTransformer - begin\", X.shape)\n",
    "        numeric_cols = X.select_dtypes(include=['number']).columns\n",
    "        categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "        self.imputer_numeric.fit(X[numeric_cols])\n",
    "        self.imputer_categorical.fit(X[categorical_cols])\n",
    "        print(\"fit - ImputerTransformer - end\", X.shape)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        print(\"transform - ImputerTransformer - begin\", X.shape)\n",
    "        numeric_cols = X.select_dtypes(include=['number']).columns\n",
    "        categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "        X[numeric_cols] = self.imputer_numeric.transform(X[numeric_cols])\n",
    "        X[categorical_cols] = self.imputer_categorical.transform(X[categorical_cols])\n",
    "        print(\"transform - ImputerTransformer - end\", X.shape)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        print(\"fit - DateTransformer - begin\", X.shape)\n",
    "        print(\"fit - DateTransformer - end\", X.shape)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        print(\"transform - DateTransformer\", X.shape)\n",
    "        X['Data'] = pd.to_datetime(X['Data'])\n",
    "        X['Ano'] = X['Data'].dt.year\n",
    "        X['Mes'] = X['Data'].dt.month\n",
    "        X['Dia'] = X['Data'].dt.day\n",
    "        X.drop('Data', axis=1, inplace=True)\n",
    "        print(\"transform - DateTransformer - end\", X.shape)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotEncoderTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print(\"fit - OneHotEncoderTransformer - begin\", X.shape)\n",
    "        self.encoder.fit(X[['Codigo']])\n",
    "        print(\"fit - OneHotEncoderTransformer - end\", X.shape)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        print(\"transform - OneHotEncoderTransformer - begin\", X.shape)\n",
    "        X.reset_index(drop=True, inplace=True)\n",
    "        encoded_data = self.encoder.transform(X[['Codigo']])\n",
    "        encoded_df = pd.DataFrame(encoded_data, columns=self.encoder.get_feature_names_out(['Codigo']))\n",
    "        X = pd.concat([X.drop(['Codigo'], axis=1), encoded_df], axis=1)\n",
    "        print(\"transform - OneHotEncoderTransformer - end\", X.shape)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustScalerTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.scaler = RobustScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print(\"fit - RobustScalerTransformer - begin\", X.shape)\n",
    "        self.scaler.fit(X[['Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima',\n",
    "                'Umidade Minima', 'Umidade Maxima','Precipitacao Total', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media', 'Umidade Media', \n",
    "                'Direcao Vento', 'Rajada Maxima de Vento', 'Vento Velocidade Media', 'Latitude', 'Longitude', 'Ano', 'Mes', 'Dia','Radiacao Global']])\n",
    "        print(\"fit - RobustScalerTransformer - end\", X.shape)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        print(\"transform - RobustScalerTransformer - begin\", X.shape)\n",
    "        X[['Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima','Umidade Minima',\n",
    "      'Umidade Maxima','Precipitacao Total', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media', 'Umidade Media', 'Direcao Vento',\n",
    "      'Rajada Maxima de Vento','Vento Velocidade Media', 'Latitude', 'Longitude', 'Ano', 'Mes', 'Dia', 'Radiacao Global']] = self.scaler.transform(\n",
    "          X[['Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima',\n",
    "                'Umidade Minima', 'Umidade Maxima','Precipitacao Total', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media', 'Umidade Media', \n",
    "                'Direcao Vento', 'Rajada Maxima de Vento', 'Vento Velocidade Media', 'Latitude', 'Longitude', 'Ano', 'Mes', 'Dia', 'Radiacao Global']])\n",
    "        print(\"transform - RobustScalerTransformer - end\", X.shape)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeLocalOutlier(X, y):\n",
    "    print(\"fit_resample - LocalOutlierTransformer - begin\", X.shape)\n",
    "    lof = LocalOutlierFactor(contamination=0.025)\n",
    "    outlier_mask = lof.fit_predict(X) != -1\n",
    "    X = X[outlier_mask]\n",
    "    y = y[outlier_mask]\n",
    "    print(\"fit_resample - LocalOutlierTransformer - end\",X.shape)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handleOutliersZIndex(X, threshold = 3):\n",
    "    print(\"handleOutliersZIndex - begin\", X.shape)\n",
    "    columns = ['Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima',\n",
    "                'Umidade Minima', 'Umidade Maxima', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media', 'Umidade Media', \n",
    "                'Direcao Vento', 'Rajada Maxima de Vento', 'Vento Velocidade Media', 'Radiacao Global']\n",
    "    \n",
    "    for column in columns:\n",
    "        non_zero_mask = (X[column].notna())\n",
    "\n",
    "        nz_X = X[non_zero_mask]\n",
    "        z_scores = np.abs(stats.zscore(nz_X[column]))\n",
    "        outliers_mask = (z_scores > threshold)\n",
    "\n",
    "        nz_X.loc[outliers_mask, column] = nz_X[(~outliers_mask)][column].mean()\n",
    "\n",
    "        X.loc[non_zero_mask, column] = nz_X[column]\n",
    "    \n",
    "    column = 'Precipitacao Total'\n",
    "    non_zero_mask = (X[column] != 0) & (X[column].notna())\n",
    "\n",
    "    nz_X = X[non_zero_mask]\n",
    "    z_scores = np.abs(stats.zscore(nz_X[column]))\n",
    "    outliers_mask = (z_scores > threshold)\n",
    "\n",
    "    nz_X.loc[outliers_mask, column] = nz_X[(~outliers_mask)][column].mean()\n",
    "\n",
    "    X.loc[non_zero_mask, column] = nz_X[column]\n",
    "\n",
    "    print(\"handleOutliersZIndex - end\", X.shape)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlp(data_x, data_y):\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(10,20,20,10), activation='relu', max_iter=15, random_state=42)\n",
    "    #mlp = MLPClassifier(hidden_layer_sizes=(100,200,200,200,100), activation='relu', max_iter=150, random_state=42)\n",
    "    mlp.fit(data_x, data_y)\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dt(data_x, data_y):\n",
    "    dt = DecisionTreeClassifier(class_weight='balanced', splitter='best', max_depth=None, criterion='gini')\n",
    "    dt.fit(data_x, data_y)\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rf(data_x, data_y):\n",
    "    rf = RandomForestClassifier(class_weight='balanced', max_depth=15, n_estimators=20, random_state=42)\n",
    "    rf.fit(data_x, data_y)\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sgd(data_x, data_y):\n",
    "    sgd = SGDClassifier(max_iter=3000, tol=1e-3, class_weight='balanced', random_state=42)\n",
    "    sgd.fit(data_x, data_y)\n",
    "    return sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logistic_regression(data_x, data_y):\n",
    "    lr = LogisticRegression(class_weight='balanced', max_iter=2000, random_state=42)\n",
    "    lr.fit(data_x, data_y)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bagging(data_x, data_y):\n",
    "   base_classifier = DecisionTreeClassifier(class_weight='balanced', splitter='best', max_depth=15, criterion='gini', random_state=42)\n",
    "   #base_classifier = DecisionTreeClassifier(splitter='best', max_depth=20, criterion='gini', random_state=42, min_samples_split=50)\n",
    "   bagging = BaggingClassifier(base_estimator=base_classifier, n_estimators=20, random_state=42)\n",
    "   bagging.fit(data_x, data_y)\n",
    "   return bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_svc(data_x, data_y):\n",
    "    linear_svc = LinearSVC(class_weight='balanced', random_state=42)\n",
    "    linear_svc.fit(data_x, data_y)\n",
    "    return linear_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_knn(data_x, data_y):\n",
    "    knn = KNeighborsClassifier(class_weight='balanced', n_neighbors=20)\n",
    "    knn.fit(data_x, data_y)\n",
    "    return knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nb(data_x, data_y):\n",
    "    nb = BernoulliNB()\n",
    "    nb.fit(data_x, data_y)\n",
    "    return nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ada(data_x, data_y):\n",
    "    ada = AdaBoostClassifier(n_estimators=100, algorithm=\"SAMME\", random_state=0)\n",
    "    ada.fit(data_x, data_y)\n",
    "    return ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble(data_x, data_y):\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(10,20,20,10), activation='relu', max_iter=15, random_state=42)\n",
    "    rf = RandomForestClassifier(class_weight='balanced', max_depth=15, n_estimators=20, random_state=42)\n",
    "    base_classifier = DecisionTreeClassifier(class_weight='balanced', splitter='best', max_depth=15, criterion='gini', random_state=42)\n",
    "    bagging = BaggingClassifier(base_estimator=base_classifier, n_estimators=20, random_state=42)\n",
    "    ensemble = VotingClassifier(estimators=[('mlp', mlp), ('rf', rf), ('bagging', bagging)], voting='hard')\n",
    "    ensemble.fit(data_x, data_y)\n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result_scikit(model, data_x, data_y):\n",
    "    y_pred = model.predict(data_x)\n",
    "    accuracy = accuracy_score(data_y, y_pred),\n",
    "    FP = np.sum((y_pred == 'Sim') & (data_y == 'Nao'))\n",
    "    FN = np.sum((y_pred == 'Nao') & (data_y == 'Sim'))\n",
    "    VP = np.sum((y_pred == 'Sim') & (data_y == 'Sim'))\n",
    "    VN = np.sum((y_pred == 'Nao') & (data_y == 'Nao'))\n",
    "\n",
    "    y_true  = (data_y == 'Sim').astype(int)\n",
    "    y_pred  = (y_pred == 'Sim').astype(int)\n",
    "\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy} | Precision:{precision} | Recall:{recall} | F1-score:{f1} | FP:{FP} | FN:{FN} | VP:{VP} | VN:{VN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256311\n",
      "240452\n",
      "handleOutliersZIndex - begin (192361, 22)\n",
      "handleOutliersZIndex - end (192361, 22)\n",
      "fit - ManualFeatureSelectorTransformer - begin (192361, 22)\n",
      "fit - ManualFeatureSelectorTransformer - end (192361, 22)\n",
      "transform - ManualFeatureSelectorTransformer (192361, 22)\n",
      "transform - ManualFeatureSelectorTransformer - end (192361, 20)\n",
      "fit - ImputerTransformer - begin (192361, 20)\n",
      "fit - ImputerTransformer - end (192361, 20)\n",
      "transform - ImputerTransformer - begin (192361, 20)\n",
      "transform - ImputerTransformer - end (192361, 20)\n",
      "fit - DateTransformer - begin (192361, 20)\n",
      "fit - DateTransformer - end (192361, 20)\n",
      "transform - DateTransformer (192361, 20)\n",
      "transform - DateTransformer - end (192361, 22)\n",
      "fit - RobustScalerTransformer - begin (192361, 22)\n",
      "fit - RobustScalerTransformer - end (192361, 22)\n",
      "transform - RobustScalerTransformer - begin (192361, 22)\n",
      "transform - RobustScalerTransformer - end (192361, 22)\n",
      "transform - ManualFeatureSelectorTransformer (48091, 22)\n",
      "transform - ManualFeatureSelectorTransformer - end (48091, 20)\n",
      "transform - ImputerTransformer - begin (48091, 20)\n",
      "transform - ImputerTransformer - end (48091, 20)\n",
      "transform - DateTransformer (48091, 20)\n",
      "transform - DateTransformer - end (48091, 22)\n",
      "transform - RobustScalerTransformer - begin (48091, 22)\n",
      "transform - RobustScalerTransformer - end (48091, 22)\n",
      "bagging\n",
      "training:\n",
      "transform - ManualFeatureSelectorTransformer (192361, 22)\n",
      "transform - ManualFeatureSelectorTransformer - end (192361, 20)\n",
      "transform - ImputerTransformer - begin (192361, 20)\n",
      "transform - ImputerTransformer - end (192361, 20)\n",
      "transform - DateTransformer (192361, 20)\n",
      "transform - DateTransformer - end (192361, 22)\n",
      "transform - RobustScalerTransformer - begin (192361, 22)\n",
      "transform - RobustScalerTransformer - end (192361, 22)\n",
      "Accuracy: (0.8763834665030853,) | Precision:0.7232174847724829 | Recall:0.9024085837229663 | F1-score:0.8029370084613026 | FP:18540 | FN:5239 | VP:48444 | VN:120138\n",
      "over sampled training:\n",
      "Accuracy: (0.8952321204516939,) | Precision:0.8736858611614874 | Recall:0.9240614949739685 | F1-score:0.8981678768678685 | FP:18527 | FN:10531 | VP:128147 | VN:120151\n",
      "test:\n",
      "Accuracy: (0.7313634567798548,) | Precision:0.5141590304463494 | Recall:0.6491267353336319 | F1-score:0.5738132154521163 | FP:8218 | FN:4701 | VP:8697 | VN:26475\n",
      "rf\n",
      "training:\n",
      "transform - ManualFeatureSelectorTransformer (192361, 22)\n",
      "transform - ManualFeatureSelectorTransformer - end (192361, 20)\n",
      "transform - ImputerTransformer - begin (192361, 20)\n",
      "transform - ImputerTransformer - end (192361, 20)\n",
      "transform - DateTransformer (192361, 20)\n",
      "transform - DateTransformer - end (192361, 22)\n",
      "transform - RobustScalerTransformer - begin (192361, 22)\n",
      "transform - RobustScalerTransformer - end (192361, 22)\n",
      "Accuracy: (0.8537645364704903,) | Precision:0.6866626733092758 | Recall:0.8755099379691895 | F1-score:0.769671661344469 | FP:21447 | FN:6683 | VP:47000 | VN:117231\n",
      "over sampled training:\n",
      "Accuracy: (0.8711619723387992,) | Precision:0.853113894872604 | Recall:0.8967175759673488 | F1-score:0.8743724599569687 | FP:21411 | FN:14323 | VP:124355 | VN:117267\n",
      "test:\n",
      "Accuracy: (0.7268927657981743,) | Precision:0.507622127266428 | Recall:0.6561427078668458 | F1-score:0.572405261101706 | FP:8527 | FN:4607 | VP:8791 | VN:26166\n",
      "ada\n",
      "training:\n",
      "transform - ManualFeatureSelectorTransformer (192361, 22)\n",
      "transform - ManualFeatureSelectorTransformer - end (192361, 20)\n",
      "transform - ImputerTransformer - begin (192361, 20)\n",
      "transform - ImputerTransformer - end (192361, 20)\n",
      "transform - DateTransformer (192361, 20)\n",
      "transform - DateTransformer - end (192361, 22)\n",
      "transform - RobustScalerTransformer - begin (192361, 22)\n",
      "transform - RobustScalerTransformer - end (192361, 22)\n",
      "Accuracy: (0.6767328096651608,) | Precision:0.44826369025159146 | Recall:0.6860272339474321 | F1-score:0.542226148409894 | FP:45329 | FN:16855 | VP:36828 | VN:93349\n",
      "over sampled training:\n",
      "Accuracy: (0.6795850819884913,) | Precision:0.6772953463041668 | Recall:0.6860424869121273 | F1-score:0.6816408558931316 | FP:45330 | FN:43539 | VP:95139 | VN:93348\n",
      "test:\n",
      "Accuracy: (0.6777151649996881,) | Precision:0.4490073297412747 | Recall:0.690401552470518 | F1-score:0.544133650989735 | FP:11351 | FN:4148 | VP:9250 | VN:23342\n",
      "nb\n",
      "training:\n",
      "transform - ManualFeatureSelectorTransformer (192361, 22)\n",
      "transform - ManualFeatureSelectorTransformer - end (192361, 20)\n",
      "transform - ImputerTransformer - begin (192361, 20)\n",
      "transform - ImputerTransformer - end (192361, 20)\n",
      "transform - DateTransformer (192361, 20)\n",
      "transform - DateTransformer - end (192361, 22)\n",
      "transform - RobustScalerTransformer - begin (192361, 22)\n",
      "transform - RobustScalerTransformer - end (192361, 22)\n",
      "Accuracy: (0.6296910496410395,) | Precision:0.39825143201688273 | Recall:0.6397928580742507 | F1-score:0.49092013578702876 | FP:51896 | FN:19337 | VP:34346 | VN:86782\n",
      "over sampled training:\n",
      "Accuracy: (0.6319062865054298,) | Precision:0.6303023827331979 | Recall:0.6380608315666508 | F1-score:0.6341578783285494 | FP:51900 | FN:50193 | VP:88485 | VN:86778\n",
      "test:\n",
      "Accuracy: (0.6337152481753343,) | Precision:0.402981640822712 | Recall:0.6536796536796536 | F1-score:0.49859098801628193 | FP:12975 | FN:4640 | VP:8758 | VN:21718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp\n",
      "training:\n",
      "transform - ManualFeatureSelectorTransformer (192361, 22)\n",
      "transform - ManualFeatureSelectorTransformer - end (192361, 20)\n",
      "transform - ImputerTransformer - begin (192361, 20)\n",
      "transform - ImputerTransformer - end (192361, 20)\n",
      "transform - DateTransformer (192361, 20)\n",
      "transform - DateTransformer - end (192361, 22)\n",
      "transform - RobustScalerTransformer - begin (192361, 22)\n",
      "transform - RobustScalerTransformer - end (192361, 22)\n",
      "Accuracy: (0.6957803296926092,) | Precision:0.4710833721917333 | Recall:0.7339381182124695 | F1-score:0.5738421205942325 | FP:44237 | FN:14283 | VP:39400 | VN:94441\n",
      "over sampled training:\n",
      "Accuracy: (0.706929000995111,) | Precision:0.697079164062661 | Recall:0.7319185451189085 | F1-score:0.7140741574535593 | FP:44108 | FN:37177 | VP:101501 | VN:94570\n",
      "test:\n",
      "Accuracy: (0.6929778960720301,) | Precision:0.46754048534929005 | Recall:0.7348111658456487 | F1-score:0.5714700333768684 | FP:11212 | FN:3553 | VP:9845 | VN:23481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble\n",
      "training:\n",
      "transform - ManualFeatureSelectorTransformer (192361, 22)\n",
      "transform - ManualFeatureSelectorTransformer - end (192361, 20)\n",
      "transform - ImputerTransformer - begin (192361, 20)\n",
      "transform - ImputerTransformer - end (192361, 20)\n",
      "transform - DateTransformer (192361, 20)\n",
      "transform - DateTransformer - end (192361, 22)\n",
      "transform - RobustScalerTransformer - begin (192361, 22)\n",
      "transform - RobustScalerTransformer - end (192361, 22)\n",
      "Accuracy: (0.838532758719283,) | Precision:0.6602172773756746 | Recall:0.8682823240131885 | F1-score:0.7500885069679122 | FP:23989 | FN:7071 | VP:46612 | VN:114689\n",
      "over sampled training:\n",
      "Accuracy: (0.8579298807309018,) | Precision:0.8372307901351994 | Recall:0.8886196801222978 | F1-score:0.8621601578350847 | FP:23958 | FN:15446 | VP:123232 | VN:114720\n",
      "test:\n",
      "Accuracy: (0.7275997587906261,) | Precision:0.5082557624113475 | Recall:0.6846544260337364 | F1-score:0.5834128347007569 | FP:8875 | FN:4225 | VP:9173 | VN:25818\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "\n",
    "print(data.shape[0])\n",
    "data = data.dropna(subset=['Vai Chover Amanha'])\n",
    "#data = data.dropna()\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "print(data.shape[0])\n",
    "\n",
    "X = data.drop('Vai Chover Amanha', axis=1)\n",
    "y = data['Vai Chover Amanha']\n",
    "\n",
    "raw_X_train, X_test, raw_y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=60)\n",
    "\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('manual',ManualFeatureSelectorTransformer()),\n",
    "    ('imputer',ImputerTransformer()),\n",
    "    ('date', DateTransformer()),\n",
    "    #('oht', OneHotEncoderTransformer()),\n",
    "    #('outliers', LocalOutlierTransformer()),\n",
    "    ('scaler', RobustScalerTransformer()),\n",
    "    #('PCA', PCA(n_components=18)),\n",
    "    #('over_sampler', RandomOverSampler(random_state=42)),\n",
    "    #('model', bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight='balanced', splitter='best', max_depth=15, criterion='gini', random_state=42), n_estimators=20, random_state=42)),\n",
    "])\n",
    "\n",
    "X_train = pipeline.fit_transform(handleOutliersZIndex(raw_X_train.copy(),4), raw_y_train.copy())\n",
    "y_train = raw_y_train.copy()\n",
    "\n",
    "X_test = pipeline.transform(X_test.copy())\n",
    "\n",
    "os = RandomOverSampler(random_state=42)\n",
    "X_train, y_train = os.fit_resample(X_train.copy(), y_train.copy())\n",
    "\n",
    "scikit_model = get_bagging(X_train.copy(), y_train.copy())\n",
    "print(\"bagging\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())\n",
    "\n",
    "scikit_model = get_rf(X_train.copy(), y_train.copy())\n",
    "print(\"rf\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())\n",
    "\n",
    "scikit_model = get_ada(X_train, y_train)\n",
    "print(\"ada\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())\n",
    "\n",
    "scikit_model = get_nb(X_train, y_train)\n",
    "print(\"nb\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())\n",
    "\n",
    "scikit_model = get_mlp(X_train, y_train)\n",
    "print(\"mlp\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())\n",
    "\n",
    "scikit_model = get_ensemble(X_train, y_train)\n",
    "print(\"ensemble\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ano- 2001 : 47\n",
      "ano- 2002 : 458\n",
      "ano- 2003 : 1073\n",
      "ano- 2004 : 519\n",
      "ano- 2005 : 337\n",
      "ano- 2006 : 116\n",
      "ano- 2007 : 439\n",
      "ano- 2008 : 444\n",
      "ano- 2009 : 311\n",
      "ano- 2010 : 461\n",
      "ano- 2011 : 464\n",
      "ano- 2012 : 702\n",
      "ano- 2013 : 659\n",
      "ano- 2014 : 1958\n",
      "ano- 2015 : 2179\n",
      "ano- 2016 : 1730\n",
      "ano- 2017 : 1309\n",
      "ano- 2018 : 1599\n",
      "ano- 2019 : 1280\n",
      "ano- 2020 : 2340\n",
      "ano- 2021 : 3996\n",
      "ano- 2022 : 3181\n",
      "ano- 2023 : 2590\n",
      "any: 28192\n",
      "ano- 2001 : 22\n",
      "ano- 2002 : 73\n",
      "ano- 2003 : 221\n",
      "ano- 2004 : 501\n",
      "ano- 2005 : 303\n",
      "ano- 2006 : 34\n",
      "ano- 2007 : 182\n",
      "ano- 2008 : 149\n",
      "ano- 2009 : 83\n",
      "ano- 2010 : 34\n",
      "ano- 2011 : 260\n",
      "ano- 2012 : 500\n",
      "ano- 2013 : 109\n",
      "ano- 2014 : 735\n",
      "ano- 2015 : 957\n",
      "ano- 2016 : 1160\n",
      "ano- 2017 : 722\n",
      "ano- 2018 : 251\n",
      "ano- 2019 : 345\n",
      "ano- 2020 : 686\n",
      "ano- 2021 : 1416\n",
      "ano- 2022 : 1165\n",
      "ano- 2023 : 549\n",
      "all: 10457\n",
      "ano- 2001 : 32\n",
      "ano- 2002 : 92\n",
      "ano- 2003 : 245\n",
      "ano- 2004 : 511\n",
      "ano- 2005 : 319\n",
      "ano- 2006 : 62\n",
      "ano- 2007 : 218\n",
      "ano- 2008 : 171\n",
      "ano- 2009 : 94\n",
      "ano- 2010 : 47\n",
      "ano- 2011 : 283\n",
      "ano- 2012 : 612\n",
      "ano- 2013 : 169\n",
      "ano- 2014 : 1185\n",
      "ano- 2015 : 1891\n",
      "ano- 2016 : 1352\n",
      "ano- 2017 : 876\n",
      "ano- 2018 : 743\n",
      "ano- 2019 : 541\n",
      "ano- 2020 : 1127\n",
      "ano- 2021 : 2479\n",
      "ano- 2022 : 1627\n",
      "ano- 2023 : 719\n",
      "y: 15395\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "\n",
    "data.drop('Radiacao Global', axis=1, inplace=True)\n",
    "data.drop('Cidade', axis=1, inplace=True)\n",
    "data.drop('Codigo', axis=1, inplace=True)\n",
    "data.drop('Latitude', axis=1, inplace=True)\n",
    "data.drop('Longitude', axis=1, inplace=True)\n",
    "\n",
    "data['Data'] = pd.to_datetime(data['Data'])\n",
    "data['Ano'] = data['Data'].dt.year\n",
    "data.drop('Data', axis=1, inplace=True)\n",
    "\n",
    "counter = 0\n",
    "for i in range(2001,2024):\n",
    "    filtro = data.loc[(data['Ano']  == i)].drop('Ano', axis=1)\n",
    "    counter += filtro.isna().any(axis=1).sum()\n",
    "    print('ano-',i,':', filtro.isna().any(axis=1).sum())\n",
    "print('any:',counter)\n",
    "\n",
    "counter = 0\n",
    "for i in range(2001,2024):\n",
    "    filtro = data.loc[(data['Ano']  == i)].drop('Ano', axis=1)\n",
    "    counter += filtro.isna().all(axis=1).sum()\n",
    "    print('ano-',i,':', filtro.isna().all(axis=1).sum())\n",
    "print('all:',counter)\n",
    "\n",
    "counter = 0\n",
    "for i in range(2001,2024):\n",
    "    counter += data.loc[(data['Ano']  == i)]['Vai Chover Amanha'].isna().sum()\n",
    "    print('ano-',i,':', data.loc[(data['Ano']  == i)]['Vai Chover Amanha'].isna().sum())\n",
    "print('y:',counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pressao Maxima 1394\n",
      "Pressao Minima 1282\n",
      "Temperatura Maxima 251\n",
      "Temperatura Minima 1346\n",
      "Temperatura Orvalho Maxima 3154\n",
      "Temperatura Orvalho Minima 1601\n",
      "Umidade Minima 0\n",
      "Umidade Maxima 12679\n",
      "Precipitacao Total 44144\n",
      "Pressao Media 1504\n",
      "Temperatura Media 707\n",
      "Temperatura Orvalho Media 2276\n",
      "Umidade Media 1847\n",
      "Direcao Vento 2053\n",
      "Rajada Maxima de Vento 7409\n",
      "Vento Velocidade Media 5893\n",
      "Radiacao Global 186\n",
      "87726\n",
      "Pressao Maxima 0\n",
      "Pressao Minima 0\n",
      "Temperatura Maxima 0\n",
      "Temperatura Minima 2\n",
      "Temperatura Orvalho Maxima 24\n",
      "Temperatura Orvalho Minima 23\n",
      "Umidade Minima 0\n",
      "Umidade Maxima 1339\n",
      "Precipitacao Total 3134\n",
      "Pressao Media 0\n",
      "Temperatura Media 0\n",
      "Temperatura Orvalho Media 7\n",
      "Umidade Media 44\n",
      "Direcao Vento 0\n",
      "Rajada Maxima de Vento 671\n",
      "Vento Velocidade Media 644\n",
      "Radiacao Global 69\n",
      "5957\n",
      "Precipitacao Total 8802\n",
      "Precipitacao Total 992\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "\n",
    "data = data.dropna()\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "columns = ['Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima',\n",
    "                'Umidade Minima', 'Umidade Maxima','Precipitacao Total', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media', 'Umidade Media', \n",
    "                'Direcao Vento', 'Rajada Maxima de Vento', 'Vento Velocidade Media', 'Radiacao Global']\n",
    "\n",
    "counter = 0\n",
    "for column in columns:\n",
    "    q25, q75 = np.percentile(data[column], 25), np.percentile(data[column], 75)\n",
    "    iqr = q75 - q25\n",
    "    cut_off = iqr * 1.5\n",
    "    lower_bound, upper_bound = q25 - cut_off, q75 + cut_off\n",
    "    outliers = [x for x in data[column] if x < lower_bound or x > upper_bound]\n",
    "    num_outliers = len(outliers)\n",
    "    print(column,num_outliers)\n",
    "    counter += num_outliers\n",
    "print(counter)\n",
    "\n",
    "counter = 0\n",
    "for column in columns:\n",
    "    z_scores = np.abs(stats.zscore(data[column]))\n",
    "    threshold = 4\n",
    "    outliers_mask = (z_scores > threshold)\n",
    "\n",
    "    num_outliers = outliers_mask.sum()\n",
    "    print(column,num_outliers)\n",
    "    counter += outliers_mask.sum()\n",
    "print(counter)\n",
    "\n",
    "column = 'Precipitacao Total'\n",
    "data = data[data[column] != 0]\n",
    "counter = 0\n",
    "q25, q75 = np.percentile(data[column], 25), np.percentile(data[column], 75)\n",
    "iqr = q75 - q25\n",
    "cut_off = iqr * 1.5\n",
    "lower_bound, upper_bound = q25 - cut_off, q75 + cut_off\n",
    "outliers = [x for x in data[column] if x < lower_bound or x > upper_bound]\n",
    "num_outliers = len(outliers)\n",
    "print(column,num_outliers)\n",
    "\n",
    "z_scores = np.abs(stats.zscore(data[column]))\n",
    "threshold = 4\n",
    "outliers_mask = (z_scores > threshold)\n",
    "\n",
    "num_outliers = outliers_mask.sum()\n",
    "print(column,num_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('Radiacao Global', axis=1, inplace=True)\n",
    "data.drop('Cidade', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape[0])\n",
    "data = data.dropna()\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "print(data.shape[0])\n",
    "print(data.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Data'] = pd.to_datetime(data['Data'])\n",
    "data['Ano'] = data['Data'].dt.year\n",
    "data['Mes'] = data['Data'].dt.month\n",
    "data['Dia'] = data['Data'].dt.day\n",
    "data.drop('Data', axis=1, inplace=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoded_data = encoder.fit_transform(data[['Codigo']])\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(['Codigo']))\n",
    "data = pd.concat([data.drop(['Codigo'], axis=1), encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "data[['Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima','Umidade Minima',\n",
    "      'Umidade Maxima','Precipitacao Total', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media', 'Umidade Media', 'Direcao Vento',\n",
    "      'Rajada Maxima de Vento','Vento Velocidade Media', 'Latitude', 'Longitude', 'Ano', 'Mes', 'Dia']] = scaler.fit_transform(\n",
    "          data[['Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima',\n",
    "                'Umidade Minima', 'Umidade Maxima','Precipitacao Total', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media', 'Umidade Media', \n",
    "                'Direcao Vento', 'Rajada Maxima de Vento', 'Vento Velocidade Media', 'Latitude', 'Longitude', 'Ano', 'Mes', 'Dia']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Vai Chover Amanha', axis=1)\n",
    "y = data['Vai Chover Amanha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof = LocalOutlierFactor(contamination=0.1)\n",
    "yhat = lof.fit_predict(X)\n",
    "mask = yhat != -1\n",
    "X = X[mask]\n",
    "y = y[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_X_train, X_test, raw_y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat([raw_X_train, raw_y_train], axis=1)\n",
    "\n",
    "majority_class_data = train_data[train_data['Vai Chover Amanha'] == 'Nao']\n",
    "minority_class_data = train_data[train_data['Vai Chover Amanha'] == 'Sim']\n",
    "upsampled_miNaority_class = resample(minority_class_data, replace=True, n_samples=len(majority_class_data))\n",
    "train_data = pd.concat([majority_class_data, upsampled_miNaority_class], axis=0)\n",
    "\n",
    "balanced_X_train = train_data.drop('Vai Chover Amanha', axis=1)\n",
    "balanced_y_train = train_data['Vai Chover Amanha']\n",
    "\n",
    "print(train_data['Vai Chover Amanha'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42,k_neighbors=15)\n",
    "print(raw_y_train.value_counts())\n",
    "balanced_X_train, balanced_y_train = sm.fit_resample(raw_X_train, raw_y_train)\n",
    "print(balanced_y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = BorderlineSMOTE(random_state=42)\n",
    "print(raw_y_train.value_counts())\n",
    "balanced_X_train, balanced_y_train = sm.fit_resample(raw_X_train, raw_y_train)\n",
    "print(balanced_y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SVMSMOTE(random_state=42)\n",
    "print(raw_y_train.value_counts())\n",
    "balanced_X_train, balanced_y_train = sm.fit_resample(raw_X_train, raw_y_train)\n",
    "print(balanced_y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = KMeansSMOTE(random_state=42)\n",
    "print(raw_y_train.value_counts())\n",
    "balanced_X_train, balanced_y_train = sm.fit_resample(raw_X_train, raw_y_train)\n",
    "\n",
    "print(balanced_y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(balanced_X_train.shape)\n",
    "selector = SelectKBest(k=20)\n",
    "processed_X_train = selector.fit_transform(balanced_X_train, balanced_y_train)\n",
    "processed_y_train = balanced_y_train.copy()\n",
    "print(processed_X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(balanced_X_train.shape)\n",
    "dt = DecisionTreeClassifier(splitter='best', criterion='gini')\n",
    "selector = SequentialFeatureSelector(dt, n_features_to_select=20)\n",
    "processed_X_train = selector.fit_transform(balanced_X_train, balanced_y_train)\n",
    "processed_y_train = balanced_y_train.copy()\n",
    "print(processed_X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(balanced_X_train.shape)\n",
    "rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "selector = SelectFromModel(rf)\n",
    "processed_X_train = selector.fit_transform(balanced_X_train, balanced_y_train)\n",
    "processed_y_train = balanced_y_train.copy()\n",
    "print(processed_X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(balanced_X_train.shape)\n",
    "selector = SelectKBest(k=66)\n",
    "processed_X_train = selector.fit_transform(balanced_X_train, balanced_y_train)\n",
    "processed_y_train = balanced_y_train.copy()\n",
    "print(processed_X_train.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
