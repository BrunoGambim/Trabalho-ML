{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "#from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn import FunctionSampler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, KMeansSMOTE, SVMSMOTE, ADASYN, RandomOverSampler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2, SelectPercentile, SelectFromModel, SequentialFeatureSelector, SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "from imblearn.base import SamplerMixin\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualFeatureSelectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        print(\"fit - ManualFeatureSelectorTransformer - begin\", X.shape)\n",
    "        print(\"fit - ManualFeatureSelectorTransformer - end\", X.shape)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        print(\"transform - ManualFeatureSelectorTransformer\", X.shape)\n",
    "        #X.drop('Radiacao Global', axis=1, inplace=True)\n",
    "        X.drop('Cidade', axis=1, inplace=True)\n",
    "        X.drop('Codigo', axis=1, inplace=True)\n",
    "        print(\"transform - ManualFeatureSelectorTransformer - end\", X.shape)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImputerTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.imputer_numeric = SimpleImputer(strategy='mean')\n",
    "        self.imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print(\"fit - ImputerTransformer - begin\", X.shape)\n",
    "        numeric_cols = X.select_dtypes(include=['number']).columns\n",
    "        categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "        self.imputer_numeric.fit(X[numeric_cols])\n",
    "        self.imputer_categorical.fit(X[categorical_cols])\n",
    "        print(\"fit - ImputerTransformer - end\", X.shape)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        print(\"transform - ImputerTransformer - begin\", X.shape)\n",
    "        numeric_cols = X.select_dtypes(include=['number']).columns\n",
    "        categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "        X[numeric_cols] = self.imputer_numeric.transform(X[numeric_cols])\n",
    "        X[categorical_cols] = self.imputer_categorical.transform(X[categorical_cols])\n",
    "        print(\"transform - ImputerTransformer - end\", X.shape)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        print(\"fit - DateTransformer - begin\", X.shape)\n",
    "        print(\"fit - DateTransformer - end\", X.shape)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        print(\"transform - DateTransformer\", X.shape)\n",
    "        X['Data'] = pd.to_datetime(X['Data'])\n",
    "        X['Ano'] = X['Data'].dt.year\n",
    "        X['Mes'] = X['Data'].dt.month\n",
    "        X['Dia'] = X['Data'].dt.day\n",
    "        X.drop('Data', axis=1, inplace=True)\n",
    "        print(\"transform - DateTransformer - end\", X.shape)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotEncoderTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print(\"fit - OneHotEncoderTransformer - begin\", X.shape)\n",
    "        self.encoder.fit(X[['Codigo']])\n",
    "        print(\"fit - OneHotEncoderTransformer - end\", X.shape)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        print(\"transform - OneHotEncoderTransformer - begin\", X.shape)\n",
    "        X.reset_index(drop=True, inplace=True)\n",
    "        encoded_data = self.encoder.transform(X[['Codigo']])\n",
    "        encoded_df = pd.DataFrame(encoded_data, columns=self.encoder.get_feature_names_out(['Codigo']))\n",
    "        X = pd.concat([X.drop(['Codigo'], axis=1), encoded_df], axis=1)\n",
    "        print(\"transform - OneHotEncoderTransformer - end\", X.shape)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustScalerTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.rainfall_scaler = RobustScaler(quantile_range=(6.5, 93.5))\n",
    "        self.scaler = RobustScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print(\"fit - RobustScalerTransformer - begin\", X.shape)\n",
    "        self.scaler.fit(X[['Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima',\n",
    "                'Umidade Minima', 'Umidade Maxima', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media', 'Umidade Media', \n",
    "                'Direcao Vento 0H','Direcao Vento 6H','Direcao Vento 12H','Direcao Vento 18H', 'Rajada Maxima de Vento 0H', 'Rajada Maxima de Vento 6H',\n",
    "                'Rajada Maxima de Vento 12H', 'Rajada Maxima de Vento 18H', 'Vento Velocidade Media 0H', 'Vento Velocidade Media 6H',\n",
    "                'Vento Velocidade Media 12H', 'Vento Velocidade Media 18H', 'Latitude', 'Longitude', 'Ano', 'Mes', 'Dia','Radiacao Global']])\n",
    "        self.rainfall_scaler.fit(X[['Precipitacao Total']])\n",
    "        print(\"fit - RobustScalerTransformer - end\", X.shape)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        print(\"transform - RobustScalerTransformer - begin\", X.shape)\n",
    "        X[['Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima','Umidade Minima',\n",
    "      'Umidade Maxima', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media', 'Umidade Media', 'Direcao Vento 0H','Direcao Vento 6H','Direcao Vento 12H', \n",
    "      'Direcao Vento 18H', 'Rajada Maxima de Vento 0H', 'Rajada Maxima de Vento 6H', 'Rajada Maxima de Vento 12H', 'Rajada Maxima de Vento 18H',\n",
    "      'Vento Velocidade Media 0H', 'Vento Velocidade Media 6H', 'Vento Velocidade Media 12H', 'Vento Velocidade Media 18H', 'Latitude', 'Longitude', 'Ano', 'Mes', 'Dia',\n",
    "      'Radiacao Global']] = self.scaler.transform(\n",
    "          X[['Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima',\n",
    "                'Umidade Minima', 'Umidade Maxima', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media', 'Umidade Media', 'Direcao Vento 0H', \n",
    "                'Direcao Vento 6H','Direcao Vento 12H','Direcao Vento 18H', 'Rajada Maxima de Vento 0H', 'Rajada Maxima de Vento 6H', 'Rajada Maxima de Vento 12H',\n",
    "                'Rajada Maxima de Vento 18H', 'Vento Velocidade Media 0H', 'Vento Velocidade Media 6H', 'Vento Velocidade Media 12H', 'Vento Velocidade Media 18H',\n",
    "                'Latitude', 'Longitude', 'Ano', 'Mes', 'Dia', 'Radiacao Global']])\n",
    "        X['Precipitacao Total'] = self.rainfall_scaler.transform(X[['Precipitacao Total']])\n",
    "        print(\"transform - RobustScalerTransformer - end\", X.shape)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeLocalOutlier(X, y):\n",
    "    print(\"fit_resample - LocalOutlierTransformer - begin\", X.shape)\n",
    "    lof = LocalOutlierFactor(contamination=0.025)\n",
    "    outlier_mask = lof.fit_predict(X) != -1\n",
    "    X = X[outlier_mask]\n",
    "    y = y[outlier_mask]\n",
    "    print(\"fit_resample - LocalOutlierTransformer - end\",X.shape)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handleOutliersZIndex(X, threshold = 3):\n",
    "    print(\"handleOutliersZIndex - begin\", X.shape)\n",
    "    columns = ['Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima',\n",
    "                'Umidade Minima', 'Umidade Maxima', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media', 'Umidade Media', \n",
    "                'Direcao Vento 0H','Direcao Vento 6H','Direcao Vento 12H','Direcao Vento 18H', 'Rajada Maxima de Vento 0H', 'Rajada Maxima de Vento 6H', \n",
    "                'Rajada Maxima de Vento 12H', 'Rajada Maxima de Vento 18H', 'Vento Velocidade Media 0H', 'Vento Velocidade Media 6H', 'Vento Velocidade Media 12H', \n",
    "                'Vento Velocidade Media 18H', 'Radiacao Global']\n",
    "    \n",
    "    for column in columns:\n",
    "        non_zero_mask = (X[column].notna())\n",
    "\n",
    "        nz_X = X[non_zero_mask]\n",
    "        z_scores = np.abs(stats.zscore(nz_X[column]))\n",
    "        outliers_mask = (z_scores > threshold)\n",
    "\n",
    "        nz_X.loc[outliers_mask, column] = nz_X[(~outliers_mask)][column].mean()\n",
    "\n",
    "        X.loc[non_zero_mask, column] = nz_X[column]\n",
    "    \n",
    "    column = 'Precipitacao Total'\n",
    "    non_zero_mask = (X[column] != 0) & (X[column].notna())\n",
    "\n",
    "    nz_X = X[non_zero_mask]\n",
    "    z_scores = np.abs(stats.zscore(nz_X[column]))\n",
    "    outliers_mask = (z_scores > threshold)\n",
    "\n",
    "    nz_X.loc[outliers_mask, column] = nz_X[(~outliers_mask)][column].mean()\n",
    "\n",
    "    X.loc[non_zero_mask, column] = nz_X[column]\n",
    "\n",
    "    print(\"handleOutliersZIndex - end\", X.shape)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlp(data_x, data_y):\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(10,20,20,10), activation='relu', max_iter=15, random_state=42)\n",
    "    #mlp = MLPClassifier(hidden_layer_sizes=(100,200,200,200,100), activation='relu', max_iter=150, random_state=42)\n",
    "    mlp.fit(data_x, data_y)\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dt(data_x, data_y):\n",
    "    dt = DecisionTreeClassifier(class_weight='balanced', splitter='best', max_depth=None, criterion='gini')\n",
    "    dt.fit(data_x, data_y)\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rf(data_x, data_y):\n",
    "    rf = RandomForestClassifier(class_weight='balanced', max_depth=15, n_estimators=20, random_state=42)\n",
    "    rf.fit(data_x, data_y)\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sgd(data_x, data_y):\n",
    "    sgd = SGDClassifier(max_iter=3000, tol=1e-3, class_weight='balanced', random_state=42)\n",
    "    sgd.fit(data_x, data_y)\n",
    "    return sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logistic_regression(data_x, data_y):\n",
    "    lr = LogisticRegression(class_weight='balanced', max_iter=2000, random_state=42)\n",
    "    lr.fit(data_x, data_y)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bagging(data_x, data_y):\n",
    "   base_classifier = DecisionTreeClassifier(class_weight='balanced', splitter='best', max_depth=15, criterion='gini', random_state=42)\n",
    "   #base_classifier = DecisionTreeClassifier(splitter='best', max_depth=20, criterion='gini', random_state=42, min_samples_split=50)\n",
    "   bagging = BaggingClassifier(base_estimator=base_classifier, n_estimators=20, random_state=42)\n",
    "   bagging.fit(data_x, data_y)\n",
    "   return bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_svc(data_x, data_y):\n",
    "    linear_svc = LinearSVC(class_weight='balanced', random_state=42)\n",
    "    linear_svc.fit(data_x, data_y)\n",
    "    return linear_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_knn(data_x, data_y):\n",
    "    knn = KNeighborsClassifier(class_weight='balanced', n_neighbors=20)\n",
    "    knn.fit(data_x, data_y)\n",
    "    return knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nb(data_x, data_y):\n",
    "    nb = BernoulliNB()\n",
    "    nb.fit(data_x, data_y)\n",
    "    return nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ada(data_x, data_y):\n",
    "    ada = AdaBoostClassifier(n_estimators=100, algorithm=\"SAMME\", random_state=0)\n",
    "    ada.fit(data_x, data_y)\n",
    "    return ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble(data_x, data_y):\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(10,20,20,10), activation='relu', max_iter=15, random_state=42)\n",
    "    rf = RandomForestClassifier(class_weight='balanced', max_depth=15, n_estimators=20, random_state=42)\n",
    "    base_classifier = DecisionTreeClassifier(class_weight='balanced', splitter='best', max_depth=15, criterion='gini', random_state=42)\n",
    "    bagging = BaggingClassifier(base_estimator=base_classifier, n_estimators=20, random_state=42)\n",
    "    ensemble = VotingClassifier(estimators=[('mlp', mlp), ('rf', rf), ('bagging', bagging)], voting='hard')\n",
    "    ensemble.fit(data_x, data_y)\n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result_scikit(model, data_x, data_y):\n",
    "    y_pred = model.predict(data_x)\n",
    "    accuracy = accuracy_score(data_y, y_pred),\n",
    "    FP = np.sum((y_pred == 'Sim') & (data_y == 'Nao'))\n",
    "    FN = np.sum((y_pred == 'Nao') & (data_y == 'Sim'))\n",
    "    VP = np.sum((y_pred == 'Sim') & (data_y == 'Sim'))\n",
    "    VN = np.sum((y_pred == 'Nao') & (data_y == 'Nao'))\n",
    "\n",
    "    y_true  = (data_y == 'Sim').astype(int)\n",
    "    y_pred  = (y_pred == 'Sim').astype(int)\n",
    "\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy} | Precision:{precision} | Recall:{recall} | F1-score:{f1} | FP:{FP} | FN:{FN} | VP:{VP} | VN:{VN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "\n",
    "print(data.shape[0])\n",
    "data = data.dropna(subset=['Vai Chover Amanha'])\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "print(data.shape[0])\n",
    "\n",
    "X = data.drop('Vai Chover Amanha', axis=1)\n",
    "y = data['Vai Chover Amanha']\n",
    "\n",
    "raw_X_train, X_test, raw_y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=60)\n",
    "\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('manual',ManualFeatureSelectorTransformer()),\n",
    "    ('imputer',ImputerTransformer()),\n",
    "    ('date', DateTransformer()),\n",
    "    #('oht', OneHotEncoderTransformer()),\n",
    "    #('outliers', LocalOutlierTransformer()),\n",
    "    ('scaler', RobustScalerTransformer()),\n",
    "    #('PCA', PCA(n_components=18)),\n",
    "    #('over_sampler', RandomOverSampler(random_state=42)),\n",
    "    #('model', bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight='balanced', splitter='best', max_depth=15, criterion='gini', random_state=42), n_estimators=20, random_state=42)),\n",
    "])\n",
    "\n",
    "X_train = pipeline.fit_transform(handleOutliersZIndex(raw_X_train.copy(),4), raw_y_train.copy())\n",
    "y_train = raw_y_train.copy()\n",
    "\n",
    "X_test = pipeline.transform(X_test.copy())\n",
    "\n",
    "os = RandomOverSampler(random_state=42)\n",
    "X_train, y_train = os.fit_resample(X_train.copy(), y_train.copy())\n",
    "\n",
    "scikit_model = get_bagging(X_train.copy(), y_train.copy())\n",
    "print(\"bagging\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())\n",
    "\n",
    "scikit_model = get_rf(X_train.copy(), y_train.copy())\n",
    "print(\"rf\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())\n",
    "\n",
    "scikit_model = get_ada(X_train, y_train)\n",
    "print(\"ada\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())\n",
    "\n",
    "scikit_model = get_nb(X_train, y_train)\n",
    "print(\"nb\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())\n",
    "\n",
    "scikit_model = get_mlp(X_train, y_train)\n",
    "print(\"mlp\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())\n",
    "\n",
    "#scikit_model = get_ensemble(X_train, y_train)\n",
    "#print(\"ensemble\")\n",
    "#print(\"training:\")\n",
    "#plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "#print(\"over sampled training:\")\n",
    "#plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "#print(\"test:\")\n",
    "#plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "\n",
    "data = data.dropna(subset=['Vai Chover Amanha'])\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X = data.drop('Vai Chover Amanha', axis=1)\n",
    "y = data['Vai Chover Amanha']\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('manual',ManualFeatureSelectorTransformer()),\n",
    "    ('imputer',ImputerTransformer()),\n",
    "    ('date', DateTransformer()),\n",
    "    ('scaler', RobustScalerTransformer()),\n",
    "])\n",
    "\n",
    "X_train = pipeline.fit_transform(handleOutliersZIndex(X,4), y)\n",
    "print(X_train.describe())\n",
    "print(X_train.shape)\n",
    "\n",
    "nums = np.arange(32)\n",
    "var_ratio = []\n",
    "for num in nums:\n",
    "  pca = PCA(n_components=num)\n",
    "  pca.fit(X_train)\n",
    "  var_ratio.append(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "print(nums)\n",
    "print(var_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "\n",
    "data.drop('Radiacao Global', axis=1, inplace=True)\n",
    "data.drop('Cidade', axis=1, inplace=True)\n",
    "data.drop('Codigo', axis=1, inplace=True)\n",
    "data.drop('Latitude', axis=1, inplace=True)\n",
    "data.drop('Longitude', axis=1, inplace=True)\n",
    "\n",
    "data['Data'] = pd.to_datetime(data['Data'])\n",
    "data['Ano'] = data['Data'].dt.year\n",
    "data.drop('Data', axis=1, inplace=True)\n",
    "\n",
    "counter = 0\n",
    "for i in range(2001,2024):\n",
    "    filtro = data.loc[(data['Ano']  == i)].drop('Ano', axis=1)\n",
    "    counter += filtro.isna().any(axis=1).sum()\n",
    "    print('ano-',i,':', filtro.isna().any(axis=1).sum())\n",
    "print('any:',counter)\n",
    "\n",
    "counter = 0\n",
    "for i in range(2001,2024):\n",
    "    filtro = data.loc[(data['Ano']  == i)].drop('Ano', axis=1)\n",
    "    counter += filtro.isna().all(axis=1).sum()\n",
    "    print('ano-',i,':', filtro.isna().all(axis=1).sum())\n",
    "print('all:',counter)\n",
    "\n",
    "counter = 0\n",
    "for i in range(2001,2024):\n",
    "    counter += data.loc[(data['Ano']  == i)]['Vai Chover Amanha'].isna().sum()\n",
    "    print('ano-',i,':', data.loc[(data['Ano']  == i)]['Vai Chover Amanha'].isna().sum())\n",
    "print('y:',counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "\n",
    "data = data.dropna()\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "columns = ['Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima',\n",
    "                'Umidade Minima', 'Umidade Maxima','Precipitacao Total', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media', 'Umidade Media', \n",
    "                'Direcao Vento 0H','Direcao Vento 6H','Direcao Vento 12H','Direcao Vento 18H', 'Rajada Maxima de Vento 0H', 'Rajada Maxima de Vento 6H',\n",
    "                'Rajada Maxima de Vento 12H', 'Rajada Maxima de Vento 18H', 'Vento Velocidade Media 0H', 'Vento Velocidade Media 6H',\n",
    "                'Vento Velocidade Media 12H', 'Vento Velocidade Media 18H', 'Radiacao Global']\n",
    "\n",
    "counter = 0\n",
    "for column in columns:\n",
    "    q25, q75 = np.percentile(data[column], 25), np.percentile(data[column], 75)\n",
    "    iqr = q75 - q25\n",
    "    cut_off = iqr * 1.5\n",
    "    lower_bound, upper_bound = q25 - cut_off, q75 + cut_off\n",
    "    outliers = [x for x in data[column] if x < lower_bound or x > upper_bound]\n",
    "    num_outliers = len(outliers)\n",
    "    print(column,num_outliers)\n",
    "    counter += num_outliers\n",
    "print(counter)\n",
    "\n",
    "counter = 0\n",
    "for column in columns:\n",
    "    z_scores = np.abs(stats.zscore(data[column]))\n",
    "    threshold = 4\n",
    "    outliers_mask = (z_scores > threshold)\n",
    "\n",
    "    num_outliers = outliers_mask.sum()\n",
    "    print(column,num_outliers)\n",
    "    counter += outliers_mask.sum()\n",
    "print(counter)\n",
    "\n",
    "column = 'Precipitacao Total'\n",
    "data = data[data[column] != 0]\n",
    "counter = 0\n",
    "q25, q75 = np.percentile(data[column], 25), np.percentile(data[column], 75)\n",
    "iqr = q75 - q25\n",
    "cut_off = iqr * 1.5\n",
    "lower_bound, upper_bound = q25 - cut_off, q75 + cut_off\n",
    "outliers = [x for x in data[column] if x < lower_bound or x > upper_bound]\n",
    "num_outliers = len(outliers)\n",
    "print(column,num_outliers)\n",
    "\n",
    "z_scores = np.abs(stats.zscore(data[column]))\n",
    "threshold = 4\n",
    "outliers_mask = (z_scores > threshold)\n",
    "\n",
    "num_outliers = outliers_mask.sum()\n",
    "print(column,num_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "\n",
    "print(data['Vai Chover Amanha'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in data.dropna()['Codigo'].unique():\n",
    "    filtered = data[(data['Codigo'] == value)]\n",
    "    lat_mode = filtered['Latitude'].mode()[0]\n",
    "    long_mode = filtered['Longitude'].mode()[0]\n",
    "    lat_outliers = (filtered['Latitude'] != lat_mode).sum()\n",
    "    long_outliers = (filtered['Longitude'] != long_mode).sum()\n",
    "    print(\"cod - \", value, \" lat: \", lat_mode, \" outliers: \", lat_outliers, \" - long: \", long_mode,\" outliers: \", long_outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42,k_neighbors=15)\n",
    "\n",
    "sm = BorderlineSMOTE(random_state=42)\n",
    "\n",
    "sm = SVMSMOTE(random_state=42)\n",
    "\n",
    "sm = KMeansSMOTE(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(k=20)\n",
    "\n",
    "dt = DecisionTreeClassifier(splitter='best', criterion='gini')\n",
    "selector = SequentialFeatureSelector(dt, n_features_to_select=20)\n",
    "\n",
    "rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "selector = SelectFromModel(rf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
