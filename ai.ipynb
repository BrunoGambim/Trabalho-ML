{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "#from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn import FunctionSampler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, KMeansSMOTE, SVMSMOTE, ADASYN, RandomOverSampler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2, SelectPercentile, SelectFromModel, SequentialFeatureSelector, SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "from imblearn.base import SamplerMixin\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualFeatureSelectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        print(\"fit - ManualFeatureSelectorTransformer - begin\", X.shape)\n",
    "        print(\"fit - ManualFeatureSelectorTransformer - end\", X.shape)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        print(\"transform - ManualFeatureSelectorTransformer\", X.shape)\n",
    "        #X.drop('Radiacao Global', axis=1, inplace=True)\n",
    "        X.drop('Cidade', axis=1, inplace=True)\n",
    "        X.drop('Codigo', axis=1, inplace=True)\n",
    "        print(\"transform - ManualFeatureSelectorTransformer - end\", X.shape)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImputerTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.imputer_numeric = SimpleImputer(strategy='mean')\n",
    "        self.imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print(\"fit - ImputerTransformer - begin\", X.shape)\n",
    "        numeric_cols = X.select_dtypes(include=['number']).columns\n",
    "        categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "        self.imputer_numeric.fit(X[numeric_cols])\n",
    "        self.imputer_categorical.fit(X[categorical_cols])\n",
    "        print(\"fit - ImputerTransformer - end\", X.shape)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        print(\"transform - ImputerTransformer - begin\", X.shape)\n",
    "        numeric_cols = X.select_dtypes(include=['number']).columns\n",
    "        categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "        X[numeric_cols] = self.imputer_numeric.transform(X[numeric_cols])\n",
    "        X[categorical_cols] = self.imputer_categorical.transform(X[categorical_cols])\n",
    "        print(\"transform - ImputerTransformer - end\", X.shape)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        print(\"fit - DateTransformer - begin\", X.shape)\n",
    "        print(\"fit - DateTransformer - end\", X.shape)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        print(\"transform - DateTransformer\", X.shape)\n",
    "        X['Data'] = pd.to_datetime(X['Data'])\n",
    "        X['Ano'] = X['Data'].dt.year\n",
    "        X['Mes'] = X['Data'].dt.month\n",
    "        X['Dia'] = X['Data'].dt.day\n",
    "        X.drop('Data', axis=1, inplace=True)\n",
    "        print(\"transform - DateTransformer - end\", X.shape)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotEncoderTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print(\"fit - OneHotEncoderTransformer - begin\", X.shape)\n",
    "        self.encoder.fit(X[['Codigo']])\n",
    "        print(\"fit - OneHotEncoderTransformer - end\", X.shape)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        print(\"transform - OneHotEncoderTransformer - begin\", X.shape)\n",
    "        X.reset_index(drop=True, inplace=True)\n",
    "        encoded_data = self.encoder.transform(X[['Codigo']])\n",
    "        encoded_df = pd.DataFrame(encoded_data, columns=self.encoder.get_feature_names_out(['Codigo']))\n",
    "        X = pd.concat([X.drop(['Codigo'], axis=1), encoded_df], axis=1)\n",
    "        print(\"transform - OneHotEncoderTransformer - end\", X.shape)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustScalerTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.rainfall_scaler = RobustScaler(quantile_range=(6.5, 93.5))\n",
    "        self.scaler = RobustScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print(\"fit - RobustScalerTransformer - begin\", X.shape)\n",
    "        self.scaler.fit(X[['Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima',\n",
    "                'Umidade Minima', 'Umidade Maxima', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media', 'Umidade Media', \n",
    "                'Direcao Vento', 'Rajada Maxima de Vento', 'Vento Velocidade Media', 'Latitude', 'Longitude', 'Ano', 'Mes', 'Dia','Radiacao Global']])\n",
    "        self.rainfall_scaler.fit(X[['Precipitacao Total']])\n",
    "        print(\"fit - RobustScalerTransformer - end\", X.shape)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        print(\"transform - RobustScalerTransformer - begin\", X.shape)\n",
    "        X[['Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima','Umidade Minima',\n",
    "      'Umidade Maxima', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media', 'Umidade Media', 'Direcao Vento',\n",
    "      'Rajada Maxima de Vento','Vento Velocidade Media', 'Latitude', 'Longitude', 'Ano', 'Mes', 'Dia', 'Radiacao Global']] = self.scaler.transform(\n",
    "          X[['Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima',\n",
    "                'Umidade Minima', 'Umidade Maxima', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media', 'Umidade Media', \n",
    "                'Direcao Vento', 'Rajada Maxima de Vento', 'Vento Velocidade Media', 'Latitude', 'Longitude', 'Ano', 'Mes', 'Dia', 'Radiacao Global']])\n",
    "        X['Precipitacao Total'] = self.rainfall_scaler.transform(X[['Precipitacao Total']])\n",
    "        print(\"transform - RobustScalerTransformer - end\", X.shape)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeLocalOutlier(X, y):\n",
    "    print(\"fit_resample - LocalOutlierTransformer - begin\", X.shape)\n",
    "    lof = LocalOutlierFactor(contamination=0.025)\n",
    "    outlier_mask = lof.fit_predict(X) != -1\n",
    "    X = X[outlier_mask]\n",
    "    y = y[outlier_mask]\n",
    "    print(\"fit_resample - LocalOutlierTransformer - end\",X.shape)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handleOutliersZIndex(X, threshold = 3):\n",
    "    print(\"handleOutliersZIndex - begin\", X.shape)\n",
    "    columns = ['Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima',\n",
    "                'Umidade Minima', 'Umidade Maxima', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media', 'Umidade Media', \n",
    "                'Direcao Vento', 'Rajada Maxima de Vento', 'Vento Velocidade Media', 'Radiacao Global']\n",
    "    \n",
    "    for column in columns:\n",
    "        non_zero_mask = (X[column].notna())\n",
    "\n",
    "        nz_X = X[non_zero_mask]\n",
    "        z_scores = np.abs(stats.zscore(nz_X[column]))\n",
    "        outliers_mask = (z_scores > threshold)\n",
    "\n",
    "        nz_X.loc[outliers_mask, column] = nz_X[(~outliers_mask)][column].mean()\n",
    "\n",
    "        X.loc[non_zero_mask, column] = nz_X[column]\n",
    "    \n",
    "    column = 'Precipitacao Total'\n",
    "    non_zero_mask = (X[column] != 0) & (X[column].notna())\n",
    "\n",
    "    nz_X = X[non_zero_mask]\n",
    "    z_scores = np.abs(stats.zscore(nz_X[column]))\n",
    "    outliers_mask = (z_scores > threshold)\n",
    "\n",
    "    nz_X.loc[outliers_mask, column] = nz_X[(~outliers_mask)][column].mean()\n",
    "\n",
    "    X.loc[non_zero_mask, column] = nz_X[column]\n",
    "\n",
    "    print(\"handleOutliersZIndex - end\", X.shape)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlp(data_x, data_y):\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(10,20,20,10), activation='relu', max_iter=15, random_state=42)\n",
    "    #mlp = MLPClassifier(hidden_layer_sizes=(100,200,200,200,100), activation='relu', max_iter=150, random_state=42)\n",
    "    mlp.fit(data_x, data_y)\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dt(data_x, data_y):\n",
    "    dt = DecisionTreeClassifier(class_weight='balanced', splitter='best', max_depth=None, criterion='gini')\n",
    "    dt.fit(data_x, data_y)\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rf(data_x, data_y):\n",
    "    rf = RandomForestClassifier(class_weight='balanced', max_depth=15, n_estimators=20, random_state=42)\n",
    "    rf.fit(data_x, data_y)\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sgd(data_x, data_y):\n",
    "    sgd = SGDClassifier(max_iter=3000, tol=1e-3, class_weight='balanced', random_state=42)\n",
    "    sgd.fit(data_x, data_y)\n",
    "    return sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logistic_regression(data_x, data_y):\n",
    "    lr = LogisticRegression(class_weight='balanced', max_iter=2000, random_state=42)\n",
    "    lr.fit(data_x, data_y)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bagging(data_x, data_y):\n",
    "   base_classifier = DecisionTreeClassifier(class_weight='balanced', splitter='best', max_depth=15, criterion='gini', random_state=42)\n",
    "   #base_classifier = DecisionTreeClassifier(splitter='best', max_depth=20, criterion='gini', random_state=42, min_samples_split=50)\n",
    "   bagging = BaggingClassifier(base_estimator=base_classifier, n_estimators=20, random_state=42)\n",
    "   bagging.fit(data_x, data_y)\n",
    "   return bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_svc(data_x, data_y):\n",
    "    linear_svc = LinearSVC(class_weight='balanced', random_state=42)\n",
    "    linear_svc.fit(data_x, data_y)\n",
    "    return linear_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_knn(data_x, data_y):\n",
    "    knn = KNeighborsClassifier(class_weight='balanced', n_neighbors=20)\n",
    "    knn.fit(data_x, data_y)\n",
    "    return knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nb(data_x, data_y):\n",
    "    nb = BernoulliNB()\n",
    "    nb.fit(data_x, data_y)\n",
    "    return nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ada(data_x, data_y):\n",
    "    ada = AdaBoostClassifier(n_estimators=100, algorithm=\"SAMME\", random_state=0)\n",
    "    ada.fit(data_x, data_y)\n",
    "    return ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble(data_x, data_y):\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(10,20,20,10), activation='relu', max_iter=15, random_state=42)\n",
    "    rf = RandomForestClassifier(class_weight='balanced', max_depth=15, n_estimators=20, random_state=42)\n",
    "    base_classifier = DecisionTreeClassifier(class_weight='balanced', splitter='best', max_depth=15, criterion='gini', random_state=42)\n",
    "    bagging = BaggingClassifier(base_estimator=base_classifier, n_estimators=20, random_state=42)\n",
    "    ensemble = VotingClassifier(estimators=[('mlp', mlp), ('rf', rf), ('bagging', bagging)], voting='hard')\n",
    "    ensemble.fit(data_x, data_y)\n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result_scikit(model, data_x, data_y):\n",
    "    y_pred = model.predict(data_x)\n",
    "    accuracy = accuracy_score(data_y, y_pred),\n",
    "    FP = np.sum((y_pred == 'Sim') & (data_y == 'Nao'))\n",
    "    FN = np.sum((y_pred == 'Nao') & (data_y == 'Sim'))\n",
    "    VP = np.sum((y_pred == 'Sim') & (data_y == 'Sim'))\n",
    "    VN = np.sum((y_pred == 'Nao') & (data_y == 'Nao'))\n",
    "\n",
    "    y_true  = (data_y == 'Sim').astype(int)\n",
    "    y_pred  = (y_pred == 'Sim').astype(int)\n",
    "\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy} | Precision:{precision} | Recall:{recall} | F1-score:{f1} | FP:{FP} | FN:{FN} | VP:{VP} | VN:{VN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256311\n",
      "240452\n",
      "handleOutliersZIndex - begin (192361, 22)\n",
      "handleOutliersZIndex - end (192361, 22)\n",
      "fit - ManualFeatureSelectorTransformer - begin (192361, 22)\n",
      "fit - ManualFeatureSelectorTransformer - end (192361, 22)\n",
      "transform - ManualFeatureSelectorTransformer (192361, 22)\n",
      "transform - ManualFeatureSelectorTransformer - end (192361, 20)\n",
      "fit - ImputerTransformer - begin (192361, 20)\n",
      "fit - ImputerTransformer - end (192361, 20)\n",
      "transform - ImputerTransformer - begin (192361, 20)\n",
      "transform - ImputerTransformer - end (192361, 20)\n",
      "fit - DateTransformer - begin (192361, 20)\n",
      "fit - DateTransformer - end (192361, 20)\n",
      "transform - DateTransformer (192361, 20)\n",
      "transform - DateTransformer - end (192361, 22)\n",
      "fit - RobustScalerTransformer - begin (192361, 22)\n",
      "fit - RobustScalerTransformer - end (192361, 22)\n",
      "transform - RobustScalerTransformer - begin (192361, 22)\n",
      "transform - RobustScalerTransformer - end (192361, 22)\n",
      "transform - ManualFeatureSelectorTransformer (48091, 22)\n",
      "transform - ManualFeatureSelectorTransformer - end (48091, 20)\n",
      "transform - ImputerTransformer - begin (48091, 20)\n",
      "transform - ImputerTransformer - end (48091, 20)\n",
      "transform - DateTransformer (48091, 20)\n",
      "transform - DateTransformer - end (48091, 22)\n",
      "transform - RobustScalerTransformer - begin (48091, 22)\n",
      "transform - RobustScalerTransformer - end (48091, 22)\n",
      "bagging\n",
      "training:\n",
      "transform - ManualFeatureSelectorTransformer (192361, 22)\n",
      "transform - ManualFeatureSelectorTransformer - end (192361, 20)\n",
      "transform - ImputerTransformer - begin (192361, 20)\n",
      "transform - ImputerTransformer - end (192361, 20)\n",
      "transform - DateTransformer (192361, 20)\n",
      "transform - DateTransformer - end (192361, 22)\n",
      "transform - RobustScalerTransformer - begin (192361, 22)\n",
      "transform - RobustScalerTransformer - end (192361, 22)\n",
      "Accuracy: (0.8763834665030853,) | Precision:0.7232174847724829 | Recall:0.9024085837229663 | F1-score:0.8029370084613026 | FP:18540 | FN:5239 | VP:48444 | VN:120138\n",
      "over sampled training:\n",
      "Accuracy: (0.8952321204516939,) | Precision:0.8736858611614874 | Recall:0.9240614949739685 | F1-score:0.8981678768678685 | FP:18527 | FN:10531 | VP:128147 | VN:120151\n",
      "test:\n",
      "Accuracy: (0.7313634567798548,) | Precision:0.5141573565052905 | Recall:0.6492013733393044 | F1-score:0.5738413326735938 | FP:8219 | FN:4700 | VP:8698 | VN:26474\n",
      "rf\n",
      "training:\n",
      "transform - ManualFeatureSelectorTransformer (192361, 22)\n",
      "transform - ManualFeatureSelectorTransformer - end (192361, 20)\n",
      "transform - ImputerTransformer - begin (192361, 20)\n",
      "transform - ImputerTransformer - end (192361, 20)\n",
      "transform - DateTransformer (192361, 20)\n",
      "transform - DateTransformer - end (192361, 22)\n",
      "transform - RobustScalerTransformer - begin (192361, 22)\n",
      "transform - RobustScalerTransformer - end (192361, 22)\n",
      "Accuracy: (0.8537697350294499,) | Precision:0.6866727054904597 | Recall:0.8755099379691895 | F1-score:0.769677963464861 | FP:21446 | FN:6683 | VP:47000 | VN:117232\n",
      "over sampled training:\n",
      "Accuracy: (0.8711655778133518,) | Precision:0.8531197475388468 | Recall:0.8967175759673488 | F1-score:0.8743755339382584 | FP:21410 | FN:14323 | VP:124355 | VN:117268\n",
      "test:\n",
      "Accuracy: (0.7269551475328024,) | Precision:0.5077091875036092 | Recall:0.6562173458725182 | F1-score:0.5724890118834446 | FP:8525 | FN:4606 | VP:8792 | VN:26168\n",
      "ada\n",
      "training:\n",
      "transform - ManualFeatureSelectorTransformer (192361, 22)\n",
      "transform - ManualFeatureSelectorTransformer - end (192361, 20)\n",
      "transform - ImputerTransformer - begin (192361, 20)\n",
      "transform - ImputerTransformer - end (192361, 20)\n",
      "transform - DateTransformer (192361, 20)\n",
      "transform - DateTransformer - end (192361, 22)\n",
      "transform - RobustScalerTransformer - begin (192361, 22)\n",
      "transform - RobustScalerTransformer - end (192361, 22)\n",
      "Accuracy: (0.6767328096651608,) | Precision:0.44826369025159146 | Recall:0.6860272339474321 | F1-score:0.542226148409894 | FP:45329 | FN:16855 | VP:36828 | VN:93349\n",
      "over sampled training:\n",
      "Accuracy: (0.6795850819884913,) | Precision:0.6772953463041668 | Recall:0.6860424869121273 | F1-score:0.6816408558931315 | FP:45330 | FN:43539 | VP:95139 | VN:93348\n",
      "test:\n",
      "Accuracy: (0.6777151649996881,) | Precision:0.4490073297412747 | Recall:0.690401552470518 | F1-score:0.544133650989735 | FP:11351 | FN:4148 | VP:9250 | VN:23342\n",
      "nb\n",
      "training:\n",
      "transform - ManualFeatureSelectorTransformer (192361, 22)\n",
      "transform - ManualFeatureSelectorTransformer - end (192361, 20)\n",
      "transform - ImputerTransformer - begin (192361, 20)\n",
      "transform - ImputerTransformer - end (192361, 20)\n",
      "transform - DateTransformer (192361, 20)\n",
      "transform - DateTransformer - end (192361, 22)\n",
      "transform - RobustScalerTransformer - begin (192361, 22)\n",
      "transform - RobustScalerTransformer - end (192361, 22)\n",
      "Accuracy: (0.6296910496410395,) | Precision:0.39825143201688273 | Recall:0.6397928580742507 | F1-score:0.49092013578702876 | FP:51896 | FN:19337 | VP:34346 | VN:86782\n",
      "over sampled training:\n",
      "Accuracy: (0.6319062865054298,) | Precision:0.6303023827331979 | Recall:0.6380608315666508 | F1-score:0.6341578783285494 | FP:51900 | FN:50193 | VP:88485 | VN:86778\n",
      "test:\n",
      "Accuracy: (0.6337152481753343,) | Precision:0.402981640822712 | Recall:0.6536796536796536 | F1-score:0.4985909880162819 | FP:12975 | FN:4640 | VP:8758 | VN:21718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Public\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp\n",
      "training:\n",
      "transform - ManualFeatureSelectorTransformer (192361, 22)\n",
      "transform - ManualFeatureSelectorTransformer - end (192361, 20)\n",
      "transform - ImputerTransformer - begin (192361, 20)\n",
      "transform - ImputerTransformer - end (192361, 20)\n",
      "transform - DateTransformer (192361, 20)\n",
      "transform - DateTransformer - end (192361, 22)\n",
      "transform - RobustScalerTransformer - begin (192361, 22)\n",
      "transform - RobustScalerTransformer - end (192361, 22)\n",
      "Accuracy: (0.7041292153814962,) | Precision:0.47997471272916586 | Recall:0.7212897937894678 | F1-score:0.5763940575783739 | FP:41952 | FN:14962 | VP:38721 | VN:96726\n",
      "over sampled training:\n",
      "Accuracy: (0.7090994966757524,) | Precision:0.7045375993679949 | Recall:0.7202512294668224 | F1-score:0.7123077636219064 | FP:41888 | FN:38795 | VP:99883 | VN:96790\n",
      "test:\n",
      "Accuracy: (0.7044769291551434,) | Precision:0.47986743173723784 | Recall:0.7240632930288102 | F1-score:0.5771999762003927 | FP:10515 | FN:3697 | VP:9701 | VN:24178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Public\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble\n",
      "training:\n",
      "transform - ManualFeatureSelectorTransformer (192361, 22)\n",
      "transform - ManualFeatureSelectorTransformer - end (192361, 20)\n",
      "transform - ImputerTransformer - begin (192361, 20)\n",
      "transform - ImputerTransformer - end (192361, 20)\n",
      "transform - DateTransformer (192361, 20)\n",
      "transform - DateTransformer - end (192361, 22)\n",
      "transform - RobustScalerTransformer - begin (192361, 22)\n",
      "transform - RobustScalerTransformer - end (192361, 22)\n",
      "Accuracy: (0.8381168740025265,) | Precision:0.6597366891997223 | Recall:0.8671832796229719 | F1-score:0.7493681889155386 | FP:24010 | FN:7130 | VP:46553 | VN:114668\n",
      "over sampled training:\n",
      "Accuracy: (0.8573169500569665,) | Precision:0.8369096669794259 | Recall:0.8876029362984756 | F1-score:0.8615112193619731 | FP:23987 | FN:15587 | VP:123091 | VN:114691\n",
      "test:\n",
      "Accuracy: (0.7289097752178162,) | Precision:0.5100933847788403 | Recall:0.6808478877444395 | F1-score:0.5832294363990922 | FP:8761 | FN:4276 | VP:9122 | VN:25932\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "\n",
    "print(data.shape[0])\n",
    "data = data.dropna(subset=['Vai Chover Amanha'])\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "print(data.shape[0])\n",
    "\n",
    "X = data.drop('Vai Chover Amanha', axis=1)\n",
    "y = data['Vai Chover Amanha']\n",
    "\n",
    "raw_X_train, X_test, raw_y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=60)\n",
    "\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('manual',ManualFeatureSelectorTransformer()),\n",
    "    ('imputer',ImputerTransformer()),\n",
    "    ('date', DateTransformer()),\n",
    "    #('oht', OneHotEncoderTransformer()),\n",
    "    #('outliers', LocalOutlierTransformer()),\n",
    "    ('scaler', RobustScalerTransformer()),\n",
    "    #('PCA', PCA(n_components=18)),\n",
    "    #('over_sampler', RandomOverSampler(random_state=42)),\n",
    "    #('model', bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight='balanced', splitter='best', max_depth=15, criterion='gini', random_state=42), n_estimators=20, random_state=42)),\n",
    "])\n",
    "\n",
    "X_train = pipeline.fit_transform(handleOutliersZIndex(raw_X_train.copy(),4), raw_y_train.copy())\n",
    "y_train = raw_y_train.copy()\n",
    "\n",
    "X_test = pipeline.transform(X_test.copy())\n",
    "\n",
    "os = RandomOverSampler(random_state=42)\n",
    "X_train, y_train = os.fit_resample(X_train.copy(), y_train.copy())\n",
    "\n",
    "scikit_model = get_bagging(X_train.copy(), y_train.copy())\n",
    "print(\"bagging\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())\n",
    "\n",
    "scikit_model = get_rf(X_train.copy(), y_train.copy())\n",
    "print(\"rf\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())\n",
    "\n",
    "scikit_model = get_ada(X_train, y_train)\n",
    "print(\"ada\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())\n",
    "\n",
    "scikit_model = get_nb(X_train, y_train)\n",
    "print(\"nb\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())\n",
    "\n",
    "scikit_model = get_mlp(X_train, y_train)\n",
    "print(\"mlp\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())\n",
    "\n",
    "scikit_model = get_ensemble(X_train, y_train)\n",
    "print(\"ensemble\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handleOutliersZIndex - begin (240452, 22)\n",
      "handleOutliersZIndex - end (240452, 22)\n",
      "fit - ManualFeatureSelectorTransformer - begin (240452, 22)\n",
      "fit - ManualFeatureSelectorTransformer - end (240452, 22)\n",
      "transform - ManualFeatureSelectorTransformer (240452, 22)\n",
      "transform - ManualFeatureSelectorTransformer - end (240452, 20)\n",
      "fit - ImputerTransformer - begin (240452, 20)\n",
      "fit - ImputerTransformer - end (240452, 20)\n",
      "transform - ImputerTransformer - begin (240452, 20)\n",
      "transform - ImputerTransformer - end (240452, 20)\n",
      "fit - DateTransformer - begin (240452, 20)\n",
      "fit - DateTransformer - end (240452, 20)\n",
      "transform - DateTransformer (240452, 20)\n",
      "transform - DateTransformer - end (240452, 22)\n",
      "fit - RobustScalerTransformer - begin (240452, 22)\n",
      "fit - RobustScalerTransformer - end (240452, 22)\n",
      "transform - RobustScalerTransformer - begin (240452, 22)\n",
      "transform - RobustScalerTransformer - end (240452, 22)\n",
      "(240452, 22)\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22]\n",
      "[0.0, 0.2534805945325313, 0.4325906356863988, 0.5855364845344709, 0.6972637345494768, 0.7522594300527919, 0.799744321477969, 0.8384121139607692, 0.870213960601051, 0.9014384617133505, 0.9251827086791016, 0.9442126507962635, 0.9617479378670514, 0.9739913922316125, 0.9850846568115978, 0.9903182904306936, 0.9941425810340763, 0.9969077119197383, 0.9988406040897168, 0.9996164751275187, 0.9998738863061465, 0.9999875516238079, 0.9999999999999999]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "\n",
    "data = data.dropna(subset=['Vai Chover Amanha'])\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X = data.drop('Vai Chover Amanha', axis=1)\n",
    "y = data['Vai Chover Amanha']\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('manual',ManualFeatureSelectorTransformer()),\n",
    "    ('imputer',ImputerTransformer()),\n",
    "    ('date', DateTransformer()),\n",
    "    ('scaler', RobustScalerTransformer()),\n",
    "])\n",
    "\n",
    "X_train = pipeline.fit_transform(handleOutliersZIndex(X,4), y)\n",
    "#print(X_train.describe())\n",
    "print(X_train.shape)\n",
    "\n",
    "nums = np.arange(23)\n",
    "var_ratio = []\n",
    "for num in nums:\n",
    "  pca = PCA(n_components=num)\n",
    "  pca.fit(X_train)\n",
    "  var_ratio.append(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "print(nums)\n",
    "print(var_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ano- 2001 : 47\n",
      "ano- 2002 : 458\n",
      "ano- 2003 : 1073\n",
      "ano- 2004 : 519\n",
      "ano- 2005 : 337\n",
      "ano- 2006 : 116\n",
      "ano- 2007 : 439\n",
      "ano- 2008 : 444\n",
      "ano- 2009 : 311\n",
      "ano- 2010 : 461\n",
      "ano- 2011 : 464\n",
      "ano- 2012 : 702\n",
      "ano- 2013 : 659\n",
      "ano- 2014 : 1958\n",
      "ano- 2015 : 2179\n",
      "ano- 2016 : 1730\n",
      "ano- 2017 : 1309\n",
      "ano- 2018 : 1599\n",
      "ano- 2019 : 1280\n",
      "ano- 2020 : 2340\n",
      "ano- 2021 : 3996\n",
      "ano- 2022 : 3181\n",
      "ano- 2023 : 2590\n",
      "any: 28192\n",
      "ano- 2001 : 22\n",
      "ano- 2002 : 73\n",
      "ano- 2003 : 221\n",
      "ano- 2004 : 501\n",
      "ano- 2005 : 303\n",
      "ano- 2006 : 34\n",
      "ano- 2007 : 182\n",
      "ano- 2008 : 149\n",
      "ano- 2009 : 83\n",
      "ano- 2010 : 34\n",
      "ano- 2011 : 260\n",
      "ano- 2012 : 500\n",
      "ano- 2013 : 109\n",
      "ano- 2014 : 735\n",
      "ano- 2015 : 957\n",
      "ano- 2016 : 1160\n",
      "ano- 2017 : 722\n",
      "ano- 2018 : 251\n",
      "ano- 2019 : 345\n",
      "ano- 2020 : 686\n",
      "ano- 2021 : 1416\n",
      "ano- 2022 : 1165\n",
      "ano- 2023 : 549\n",
      "all: 10457\n",
      "ano- 2001 : 32\n",
      "ano- 2002 : 92\n",
      "ano- 2003 : 245\n",
      "ano- 2004 : 511\n",
      "ano- 2005 : 319\n",
      "ano- 2006 : 62\n",
      "ano- 2007 : 218\n",
      "ano- 2008 : 171\n",
      "ano- 2009 : 94\n",
      "ano- 2010 : 47\n",
      "ano- 2011 : 283\n",
      "ano- 2012 : 612\n",
      "ano- 2013 : 169\n",
      "ano- 2014 : 1185\n",
      "ano- 2015 : 1891\n",
      "ano- 2016 : 1352\n",
      "ano- 2017 : 876\n",
      "ano- 2018 : 743\n",
      "ano- 2019 : 541\n",
      "ano- 2020 : 1127\n",
      "ano- 2021 : 2479\n",
      "ano- 2022 : 1627\n",
      "ano- 2023 : 719\n",
      "y: 15395\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "\n",
    "data.drop('Radiacao Global', axis=1, inplace=True)\n",
    "data.drop('Cidade', axis=1, inplace=True)\n",
    "data.drop('Codigo', axis=1, inplace=True)\n",
    "data.drop('Latitude', axis=1, inplace=True)\n",
    "data.drop('Longitude', axis=1, inplace=True)\n",
    "\n",
    "data['Data'] = pd.to_datetime(data['Data'])\n",
    "data['Ano'] = data['Data'].dt.year\n",
    "data.drop('Data', axis=1, inplace=True)\n",
    "\n",
    "counter = 0\n",
    "for i in range(2001,2024):\n",
    "    filtro = data.loc[(data['Ano']  == i)].drop('Ano', axis=1)\n",
    "    counter += filtro.isna().any(axis=1).sum()\n",
    "    print('ano-',i,':', filtro.isna().any(axis=1).sum())\n",
    "print('any:',counter)\n",
    "\n",
    "counter = 0\n",
    "for i in range(2001,2024):\n",
    "    filtro = data.loc[(data['Ano']  == i)].drop('Ano', axis=1)\n",
    "    counter += filtro.isna().all(axis=1).sum()\n",
    "    print('ano-',i,':', filtro.isna().all(axis=1).sum())\n",
    "print('all:',counter)\n",
    "\n",
    "counter = 0\n",
    "for i in range(2001,2024):\n",
    "    counter += data.loc[(data['Ano']  == i)]['Vai Chover Amanha'].isna().sum()\n",
    "    print('ano-',i,':', data.loc[(data['Ano']  == i)]['Vai Chover Amanha'].isna().sum())\n",
    "print('y:',counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pressao Maxima 1394\n",
      "Pressao Minima 1282\n",
      "Temperatura Maxima 251\n",
      "Temperatura Minima 1346\n",
      "Temperatura Orvalho Maxima 3154\n",
      "Temperatura Orvalho Minima 1601\n",
      "Umidade Minima 0\n",
      "Umidade Maxima 12679\n",
      "Precipitacao Total 44144\n",
      "Pressao Media 1504\n",
      "Temperatura Media 707\n",
      "Temperatura Orvalho Media 2276\n",
      "Umidade Media 1847\n",
      "Direcao Vento 2053\n",
      "Rajada Maxima de Vento 7409\n",
      "Vento Velocidade Media 5893\n",
      "Radiacao Global 186\n",
      "87726\n",
      "Pressao Maxima 0\n",
      "Pressao Minima 0\n",
      "Temperatura Maxima 0\n",
      "Temperatura Minima 2\n",
      "Temperatura Orvalho Maxima 24\n",
      "Temperatura Orvalho Minima 23\n",
      "Umidade Minima 0\n",
      "Umidade Maxima 1339\n",
      "Precipitacao Total 3134\n",
      "Pressao Media 0\n",
      "Temperatura Media 0\n",
      "Temperatura Orvalho Media 7\n",
      "Umidade Media 44\n",
      "Direcao Vento 0\n",
      "Rajada Maxima de Vento 671\n",
      "Vento Velocidade Media 644\n",
      "Radiacao Global 69\n",
      "5957\n",
      "Precipitacao Total 8802\n",
      "Precipitacao Total 992\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "\n",
    "data = data.dropna()\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "columns = ['Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima',\n",
    "                'Umidade Minima', 'Umidade Maxima','Precipitacao Total', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media', 'Umidade Media', \n",
    "                'Direcao Vento', 'Rajada Maxima de Vento', 'Vento Velocidade Media', 'Radiacao Global']\n",
    "\n",
    "counter = 0\n",
    "for column in columns:\n",
    "    q25, q75 = np.percentile(data[column], 25), np.percentile(data[column], 75)\n",
    "    iqr = q75 - q25\n",
    "    cut_off = iqr * 1.5\n",
    "    lower_bound, upper_bound = q25 - cut_off, q75 + cut_off\n",
    "    outliers = [x for x in data[column] if x < lower_bound or x > upper_bound]\n",
    "    num_outliers = len(outliers)\n",
    "    print(column,num_outliers)\n",
    "    counter += num_outliers\n",
    "print(counter)\n",
    "\n",
    "counter = 0\n",
    "for column in columns:\n",
    "    z_scores = np.abs(stats.zscore(data[column]))\n",
    "    threshold = 4\n",
    "    outliers_mask = (z_scores > threshold)\n",
    "\n",
    "    num_outliers = outliers_mask.sum()\n",
    "    print(column,num_outliers)\n",
    "    counter += outliers_mask.sum()\n",
    "print(counter)\n",
    "\n",
    "column = 'Precipitacao Total'\n",
    "data = data[data[column] != 0]\n",
    "counter = 0\n",
    "q25, q75 = np.percentile(data[column], 25), np.percentile(data[column], 75)\n",
    "iqr = q75 - q25\n",
    "cut_off = iqr * 1.5\n",
    "lower_bound, upper_bound = q25 - cut_off, q75 + cut_off\n",
    "outliers = [x for x in data[column] if x < lower_bound or x > upper_bound]\n",
    "num_outliers = len(outliers)\n",
    "print(column,num_outliers)\n",
    "\n",
    "z_scores = np.abs(stats.zscore(data[column]))\n",
    "threshold = 4\n",
    "outliers_mask = (z_scores > threshold)\n",
    "\n",
    "num_outliers = outliers_mask.sum()\n",
    "print(column,num_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nao    173371\n",
      "Sim     67081\n",
      "Name: Vai Chover Amanha, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "\n",
    "print(data['Vai Chover Amanha'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cod -  A801  lat:  -30.05361111  outliers:  0  - long:  -51.17472221  outliers:  0\n",
      "cod -  A802  lat:  -32.07888888  outliers:  0  - long:  -52.16777777  outliers:  0\n",
      "cod -  A803  lat:  -29.72499999  outliers:  0  - long:  -53.72055554  outliers:  0\n",
      "cod -  A804  lat:  -30.75055555  outliers:  0  - long:  -55.40138888  outliers:  0\n",
      "cod -  A805  lat:  -27.85444444  outliers:  0  - long:  -53.7911111  outliers:  0\n",
      "cod -  A808  lat:  -29.35027777  outliers:  0  - long:  -49.73333333  outliers:  0\n",
      "cod -  A809  lat:  -29.83999999  outliers:  0  - long:  -57.08194443  outliers:  0\n",
      "cod -  A810  lat:  -27.89055555  outliers:  0  - long:  -54.47999999  outliers:  0\n",
      "cod -  A812  lat:  -30.54527777  outliers:  0  - long:  -53.46694443  outliers:  0\n",
      "cod -  A813  lat:  -29.87222221  outliers:  0  - long:  -52.38194443  outliers:  0\n",
      "cod -  A826  lat:  -29.70916666  outliers:  0  - long:  -55.52555554  outliers:  0\n",
      "cod -  A828  lat:  -27.65777777  outliers:  0  - long:  -52.30583333  outliers:  0\n",
      "cod -  A829  lat:  -28.7486111  outliers:  0  - long:  -50.05777777  outliers:  0\n",
      "cod -  A838  lat:  -30.80805555  outliers:  0  - long:  -51.83416666  outliers:  0\n",
      "cod -  A839  lat:  -28.22666666  outliers:  0  - long:  -52.40361111  outliers:  0\n",
      "cod -  A840  lat:  -29.164581  outliers:  0  - long:  -51.534202  outliers:  0\n",
      "cod -  A811  lat:  -31.40333333  outliers:  0  - long:  -52.70083333  outliers:  0\n",
      "cod -  A827  lat:  -31.34777777  outliers:  0  - long:  -54.01333333  outliers:  0\n",
      "cod -  A830  lat:  -28.65  outliers:  0  - long:  -56.01638888  outliers:  0\n",
      "cod -  A831  lat:  -30.368578  outliers:  0  - long:  -56.437115  outliers:  0\n",
      "cod -  A832  lat:  -30.34138888  outliers:  0  - long:  -54.31083333  outliers:  0\n",
      "cod -  A836  lat:  -32.53472221  outliers:  0  - long:  -53.37583332  outliers:  0\n",
      "cod -  A844  lat:  -28.222381  outliers:  0  - long:  -51.512845  outliers:  0\n",
      "cod -  A852  lat:  -28.41722221  outliers:  0  - long:  -54.9625  outliers:  0\n",
      "cod -  A853  lat:  -28.60344  outliers:  0  - long:  -53.673597  outliers:  0\n",
      "cod -  A854  lat:  -27.39555555  outliers:  0  - long:  -53.42944443  outliers:  0\n",
      "cod -  A834  lat:  -30.01027777  outliers:  0  - long:  -50.13583333  outliers:  0\n",
      "cod -  A837  lat:  -28.859211  outliers:  0  - long:  -52.542387  outliers:  0\n",
      "cod -  A856  lat:  -27.92027777  outliers:  0  - long:  -53.31805554  outliers:  0\n",
      "cod -  A878  lat:  -31.24833333  outliers:  0  - long:  -50.90638888  outliers:  0\n",
      "cod -  A879  lat:  -29.36888888  outliers:  0  - long:  -50.82722221  outliers:  0\n",
      "cod -  A880  lat:  -28.51361111  outliers:  0  - long:  -50.88277777  outliers:  0\n",
      "cod -  A899  lat:  -33.74222221  outliers:  0  - long:  -53.37222221  outliers:  0\n",
      "cod -  A833  lat:  -29.191599  outliers:  0  - long:  -54.885653  outliers:  0\n",
      "cod -  A881  lat:  -31.0025  outliers:  0  - long:  -54.61805554  outliers:  0\n",
      "cod -  A882  lat:  -29.44916666  outliers:  0  - long:  -51.82333332  outliers:  0\n",
      "cod -  A883  lat:  -28.65333333  outliers:  0  - long:  -53.11194444  outliers:  0\n",
      "cod -  A884  lat:  -29.674293  outliers:  0  - long:  -51.064042  outliers:  0\n",
      "cod -  A886  lat:  -29.08944444  outliers:  0  - long:  -53.82666666  outliers:  0\n",
      "cod -  A889  lat:  -29.70222222  outliers:  0  - long:  -54.69444444  outliers:  0\n",
      "cod -  A894  lat:  -28.70472222  outliers:  0  - long:  -51.87083332  outliers:  0\n",
      "cod -  A897  lat:  -29.04916666  outliers:  0  - long:  -50.14972221  outliers:  0\n",
      "cod -  A893  lat:  -30.54305555  outliers:  0  - long:  -52.52472221  outliers:  0\n",
      "cod -  A887  lat:  -31.8025  outliers:  0  - long:  -52.40722222  outliers:  0\n",
      "cod -  B807  lat:  -30.1861111  outliers:  0  - long:  -51.17805554  outliers:  0\n"
     ]
    }
   ],
   "source": [
    "for value in data.dropna()['Codigo'].unique():\n",
    "    filtered = data[(data['Codigo'] == value)]\n",
    "    lat_mode = filtered['Latitude'].mode()[0]\n",
    "    long_mode = filtered['Longitude'].mode()[0]\n",
    "    lat_outliers = (filtered['Latitude'] != lat_mode).sum()\n",
    "    long_outliers = (filtered['Longitude'] != long_mode).sum()\n",
    "    print(\"cod - \", value, \" lat: \", lat_mode, \" outliers: \", lat_outliers, \" - long: \", long_mode,\" outliers: \", long_outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42,k_neighbors=15)\n",
    "\n",
    "sm = BorderlineSMOTE(random_state=42)\n",
    "\n",
    "sm = SVMSMOTE(random_state=42)\n",
    "\n",
    "sm = KMeansSMOTE(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(k=20)\n",
    "\n",
    "dt = DecisionTreeClassifier(splitter='best', criterion='gini')\n",
    "selector = SequentialFeatureSelector(dt, n_features_to_select=20)\n",
    "\n",
    "rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "selector = SelectFromModel(rf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
