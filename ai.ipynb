{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "#from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn import FunctionSampler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, KMeansSMOTE, SVMSMOTE, ADASYN, RandomOverSampler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2, SelectPercentile, SelectFromModel, SequentialFeatureSelector, SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "from imblearn.base import SamplerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualFeatureSelectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        print(\"fit - ManualFeatureSelectorTransformer - begin\", X.shape)\n",
    "        print(\"fit - ManualFeatureSelectorTransformer - end\", X.shape)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        print(\"transform - ManualFeatureSelectorTransformer\", X.shape)\n",
    "        #X.drop('Radiacao Global', axis=1, inplace=True)\n",
    "        X.drop('Cidade', axis=1, inplace=True)\n",
    "        X.drop('Codigo', axis=1, inplace=True)\n",
    "        print(\"transform - ManualFeatureSelectorTransformer - end\", X.shape)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImputerTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.imputer_numeric = SimpleImputer(strategy='mean')\n",
    "        self.imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print(\"fit - ImputerTransformer - begin\", X.shape)\n",
    "        numeric_cols = X.select_dtypes(include=['number']).columns\n",
    "        categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "        self.imputer_numeric.fit(X[numeric_cols])\n",
    "        self.imputer_categorical.fit(X[categorical_cols])\n",
    "        print(\"fit - ImputerTransformer - end\", X.shape)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        print(\"transform - ImputerTransformer - begin\", X.shape)\n",
    "        numeric_cols = X.select_dtypes(include=['number']).columns\n",
    "        categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "        X[numeric_cols] = self.imputer_numeric.transform(X[numeric_cols])\n",
    "        X[categorical_cols] = self.imputer_categorical.transform(X[categorical_cols])\n",
    "        print(\"transform - ImputerTransformer - end\", X.shape)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        print(\"fit - DateTransformer - begin\", X.shape)\n",
    "        print(\"fit - DateTransformer - end\", X.shape)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        print(\"transform - DateTransformer\", X.shape)\n",
    "        X['Data'] = pd.to_datetime(X['Data'])\n",
    "        X['Ano'] = X['Data'].dt.year\n",
    "        X['Mes'] = X['Data'].dt.month\n",
    "        X['Dia'] = X['Data'].dt.day\n",
    "        X.drop('Data', axis=1, inplace=True)\n",
    "        print(\"transform - DateTransformer - end\", X.shape)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotEncoderTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print(\"fit - OneHotEncoderTransformer - begin\", X.shape)\n",
    "        self.encoder.fit(X[['Codigo']])\n",
    "        print(\"fit - OneHotEncoderTransformer - end\", X.shape)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        print(\"transform - OneHotEncoderTransformer - begin\", X.shape)\n",
    "        X.reset_index(drop=True, inplace=True)\n",
    "        encoded_data = self.encoder.transform(X[['Codigo']])\n",
    "        encoded_df = pd.DataFrame(encoded_data, columns=self.encoder.get_feature_names_out(['Codigo']))\n",
    "        X = pd.concat([X.drop(['Codigo'], axis=1), encoded_df], axis=1)\n",
    "        print(\"transform - OneHotEncoderTransformer - end\", X.shape)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustScalerTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.scaler = RobustScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print(\"fit - RobustScalerTransformer - begin\", X.shape)\n",
    "        self.scaler.fit(X[['Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima',\n",
    "                'Umidade Minima', 'Umidade Maxima','Precipitacao Total', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media', 'Umidade Media', \n",
    "                'Direcao Vento', 'Rajada Maxima de Vento', 'Vento Velocidade Media', 'Latitude', 'Longitude', 'Ano', 'Mes', 'Dia','Radiacao Global']])\n",
    "        print(\"fit - RobustScalerTransformer - end\", X.shape)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        print(\"transform - RobustScalerTransformer - begin\", X.shape)\n",
    "        X[['Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima','Umidade Minima',\n",
    "      'Umidade Maxima','Precipitacao Total', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media', 'Umidade Media', 'Direcao Vento',\n",
    "      'Rajada Maxima de Vento','Vento Velocidade Media', 'Latitude', 'Longitude', 'Ano', 'Mes', 'Dia', 'Radiacao Global']] = self.scaler.transform(\n",
    "          X[['Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima',\n",
    "                'Umidade Minima', 'Umidade Maxima','Precipitacao Total', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media', 'Umidade Media', \n",
    "                'Direcao Vento', 'Rajada Maxima de Vento', 'Vento Velocidade Media', 'Latitude', 'Longitude', 'Ano', 'Mes', 'Dia', 'Radiacao Global']])\n",
    "        print(\"transform - RobustScalerTransformer - end\", X.shape)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeLocalOutlier(X, y):\n",
    "    print(\"fit_resample - LocalOutlierTransformer - begin\", X.shape)\n",
    "    lof = LocalOutlierFactor(contamination=0.025)\n",
    "    outlier_mask = lof.fit_predict(X) != -1\n",
    "    X = X[outlier_mask]\n",
    "    y = y[outlier_mask]\n",
    "    print(\"fit_resample - LocalOutlierTransformer - end\",X.shape)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handleOutliersZIndex(X, threshold = 3):\n",
    "    print(\"handleOutliersZIndex - begin\", X.shape)\n",
    "    columns = ['Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima',\n",
    "                'Umidade Minima', 'Umidade Maxima', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media', 'Umidade Media', \n",
    "                'Direcao Vento', 'Rajada Maxima de Vento', 'Vento Velocidade Media', 'Radiacao Global']\n",
    "    \n",
    "    for column in columns:\n",
    "        non_zero_mask = (X[column].notna())\n",
    "\n",
    "        nz_X = X[non_zero_mask]\n",
    "        z_scores = np.abs(stats.zscore(nz_X[column]))\n",
    "        outliers_mask = (z_scores > threshold)\n",
    "\n",
    "        nz_X.loc[outliers_mask, column] = nz_X[(~outliers_mask)][column].mean()\n",
    "\n",
    "        X.loc[non_zero_mask, column] = nz_X[column]\n",
    "    \n",
    "    column = 'Precipitacao Total'\n",
    "    non_zero_mask = (X[column] != 0) & (X[column].notna())\n",
    "\n",
    "    nz_X = X[non_zero_mask]\n",
    "    z_scores = np.abs(stats.zscore(nz_X[column]))\n",
    "    outliers_mask = (z_scores > threshold)\n",
    "\n",
    "    nz_X.loc[outliers_mask, column] = nz_X[(~outliers_mask)][column].mean()\n",
    "\n",
    "    X.loc[non_zero_mask, column] = nz_X[column]\n",
    "\n",
    "    print(\"handleOutliersZIndex - end\", X.shape)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlp(data_x, data_y):\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(10,20,20,10), activation='relu', max_iter=15, random_state=42),\n",
    "    #mlp = MLPClassifier(hidden_layer_sizes=(100,200,200,200,100), activation='relu', max_iter=150, random_state=42)\n",
    "    mlp.fit(data_x, data_y)\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dt(data_x, data_y):\n",
    "    dt = DecisionTreeClassifier(class_weight='balanced', splitter='best', max_depth=None, criterion='gini')\n",
    "    dt.fit(data_x, data_y)\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rf(data_x, data_y):\n",
    "    rf = RandomForestClassifier(class_weight='balanced', max_depth=15, n_estimators=20, random_state=42)\n",
    "    rf.fit(data_x, data_y)\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sgd(data_x, data_y):\n",
    "    sgd = SGDClassifier(max_iter=3000, tol=1e-3, class_weight='balanced', random_state=42)\n",
    "    sgd.fit(data_x, data_y)\n",
    "    return sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logistic_regression(data_x, data_y):\n",
    "    lr = LogisticRegression(class_weight='balanced', max_iter=2000, random_state=42)\n",
    "    lr.fit(data_x, data_y)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bagging(data_x, data_y):\n",
    "   base_classifier = DecisionTreeClassifier(class_weight='balanced', splitter='best', max_depth=15, criterion='gini', random_state=42)\n",
    "   #base_classifier = DecisionTreeClassifier(splitter='best', max_depth=20, criterion='gini', random_state=42, min_samples_split=50)\n",
    "   bagging = BaggingClassifier(base_estimator=base_classifier, n_estimators=20, random_state=42)\n",
    "   bagging.fit(data_x, data_y)\n",
    "   return bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_svc(data_x, data_y):\n",
    "    linear_svc = LinearSVC(class_weight='balanced', random_state=42)\n",
    "    linear_svc.fit(data_x, data_y)\n",
    "    return linear_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_knn(data_x, data_y):\n",
    "    knn = KNeighborsClassifier(class_weight='balanced', n_neighbors=20)\n",
    "    knn.fit(data_x, data_y)\n",
    "    return knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nb(data_x, data_y):\n",
    "    nb = BernoulliNB()\n",
    "    nb.fit(data_x, data_y)\n",
    "    return nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ada(data_x, data_y):\n",
    "    ada = AdaBoostClassifier(n_estimators=100, algorithm=\"SAMME\", random_state=0)\n",
    "    ada.fit(data_x, data_y)\n",
    "    return ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result_scikit(model, data_x, data_y):\n",
    "    y_pred = model.predict(data_x)\n",
    "    accuracy = accuracy_score(data_y, y_pred),\n",
    "    FP = np.sum((y_pred == 'Sim') & (data_y == 'Nao'))\n",
    "    FN = np.sum((y_pred == 'Nao') & (data_y == 'Sim'))\n",
    "    VP = np.sum((y_pred == 'Sim') & (data_y == 'Sim'))\n",
    "    VN = np.sum((y_pred == 'Nao') & (data_y == 'Nao'))\n",
    "\n",
    "    y_true  = (data_y == 'Sim').astype(int)\n",
    "    y_pred  = (y_pred == 'Sim').astype(int)\n",
    "\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy} | Precision:{precision} | Recall:{recall} | F1-score:{f1} | FP:{FP} | FN:{FN} | VP:{VP} | VN:{VN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256311\n",
      "240452\n",
      "handleOutliersZIndex - begin (192361, 22)\n",
      "handleOutliersZIndex - end (192361, 22)\n",
      "fit - ManualFeatureSelectorTransformer - begin (192361, 22)\n",
      "fit - ManualFeatureSelectorTransformer - end (192361, 22)\n",
      "transform - ManualFeatureSelectorTransformer (192361, 22)\n",
      "transform - ManualFeatureSelectorTransformer - end (192361, 20)\n",
      "fit - ImputerTransformer - begin (192361, 20)\n",
      "fit - ImputerTransformer - end (192361, 20)\n",
      "transform - ImputerTransformer - begin (192361, 20)\n",
      "transform - ImputerTransformer - end (192361, 20)\n",
      "fit - DateTransformer - begin (192361, 20)\n",
      "fit - DateTransformer - end (192361, 20)\n",
      "transform - DateTransformer (192361, 20)\n",
      "transform - DateTransformer - end (192361, 22)\n",
      "fit - RobustScalerTransformer - begin (192361, 22)\n",
      "fit - RobustScalerTransformer - end (192361, 22)\n",
      "transform - RobustScalerTransformer - begin (192361, 22)\n",
      "transform - RobustScalerTransformer - end (192361, 22)\n",
      "transform - ManualFeatureSelectorTransformer (48091, 22)\n",
      "transform - ManualFeatureSelectorTransformer - end (48091, 20)\n",
      "transform - ImputerTransformer - begin (48091, 20)\n",
      "transform - ImputerTransformer - end (48091, 20)\n",
      "transform - DateTransformer (48091, 20)\n",
      "transform - DateTransformer - end (48091, 22)\n",
      "transform - RobustScalerTransformer - begin (48091, 22)\n",
      "transform - RobustScalerTransformer - end (48091, 22)\n",
      "bagging\n",
      "training:\n",
      "transform - ManualFeatureSelectorTransformer (192361, 22)\n",
      "transform - ManualFeatureSelectorTransformer - end (192361, 20)\n",
      "transform - ImputerTransformer - begin (192361, 20)\n",
      "transform - ImputerTransformer - end (192361, 20)\n",
      "transform - DateTransformer (192361, 20)\n",
      "transform - DateTransformer - end (192361, 22)\n",
      "transform - RobustScalerTransformer - begin (192361, 22)\n",
      "transform - RobustScalerTransformer - end (192361, 22)\n",
      "Accuracy: (0.8763834665030853,) | Precision:0.7232174847724829 | Recall:0.9024085837229663 | F1-score:0.8029370084613026 | FP:18540 | FN:5239 | VP:48444 | VN:120138\n",
      "over sampled training:\n",
      "Accuracy: (0.8952321204516939,) | Precision:0.8736858611614874 | Recall:0.9240614949739685 | F1-score:0.8981678768678685 | FP:18527 | FN:10531 | VP:128147 | VN:120151\n",
      "test:\n",
      "Accuracy: (0.7313634567798548,) | Precision:0.5141590304463494 | Recall:0.6491267353336319 | F1-score:0.5738132154521162 | FP:8218 | FN:4701 | VP:8697 | VN:26475\n",
      "rf\n",
      "training:\n",
      "transform - ManualFeatureSelectorTransformer (192361, 22)\n",
      "transform - ManualFeatureSelectorTransformer - end (192361, 20)\n",
      "transform - ImputerTransformer - begin (192361, 20)\n",
      "transform - ImputerTransformer - end (192361, 20)\n",
      "transform - DateTransformer (192361, 20)\n",
      "transform - DateTransformer - end (192361, 22)\n",
      "transform - RobustScalerTransformer - begin (192361, 22)\n",
      "transform - RobustScalerTransformer - end (192361, 22)\n",
      "Accuracy: (0.8537645364704903,) | Precision:0.6866626733092758 | Recall:0.8755099379691895 | F1-score:0.7696716613444691 | FP:21447 | FN:6683 | VP:47000 | VN:117231\n",
      "over sampled training:\n",
      "Accuracy: (0.8711619723387992,) | Precision:0.853113894872604 | Recall:0.8967175759673488 | F1-score:0.8743724599569687 | FP:21411 | FN:14323 | VP:124355 | VN:117267\n",
      "test:\n",
      "Accuracy: (0.7268927657981743,) | Precision:0.507622127266428 | Recall:0.6561427078668458 | F1-score:0.572405261101706 | FP:8527 | FN:4607 | VP:8791 | VN:26166\n",
      "ada\n",
      "training:\n",
      "transform - ManualFeatureSelectorTransformer (192361, 22)\n",
      "transform - ManualFeatureSelectorTransformer - end (192361, 20)\n",
      "transform - ImputerTransformer - begin (192361, 20)\n",
      "transform - ImputerTransformer - end (192361, 20)\n",
      "transform - DateTransformer (192361, 20)\n",
      "transform - DateTransformer - end (192361, 22)\n",
      "transform - RobustScalerTransformer - begin (192361, 22)\n",
      "transform - RobustScalerTransformer - end (192361, 22)\n",
      "Accuracy: (0.6767328096651608,) | Precision:0.44826369025159146 | Recall:0.6860272339474321 | F1-score:0.542226148409894 | FP:45329 | FN:16855 | VP:36828 | VN:93349\n",
      "over sampled training:\n",
      "Accuracy: (0.6795850819884913,) | Precision:0.6772953463041668 | Recall:0.6860424869121273 | F1-score:0.6816408558931315 | FP:45330 | FN:43539 | VP:95139 | VN:93348\n",
      "test:\n",
      "Accuracy: (0.6777151649996881,) | Precision:0.4490073297412747 | Recall:0.690401552470518 | F1-score:0.544133650989735 | FP:11351 | FN:4148 | VP:9250 | VN:23342\n",
      "nb\n",
      "training:\n",
      "transform - ManualFeatureSelectorTransformer (192361, 22)\n",
      "transform - ManualFeatureSelectorTransformer - end (192361, 20)\n",
      "transform - ImputerTransformer - begin (192361, 20)\n",
      "transform - ImputerTransformer - end (192361, 20)\n",
      "transform - DateTransformer (192361, 20)\n",
      "transform - DateTransformer - end (192361, 22)\n",
      "transform - RobustScalerTransformer - begin (192361, 22)\n",
      "transform - RobustScalerTransformer - end (192361, 22)\n",
      "Accuracy: (0.6296910496410395,) | Precision:0.39825143201688273 | Recall:0.6397928580742507 | F1-score:0.49092013578702876 | FP:51896 | FN:19337 | VP:34346 | VN:86782\n",
      "over sampled training:\n",
      "Accuracy: (0.6319062865054298,) | Precision:0.6303023827331979 | Recall:0.6380608315666508 | F1-score:0.6341578783285494 | FP:51900 | FN:50193 | VP:88485 | VN:86778\n",
      "test:\n",
      "Accuracy: (0.6337152481753343,) | Precision:0.402981640822712 | Recall:0.6536796536796536 | F1-score:0.4985909880162819 | FP:12975 | FN:4640 | VP:8758 | VN:21718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Public\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp\n",
      "training:\n",
      "transform - ManualFeatureSelectorTransformer (192361, 22)\n",
      "transform - ManualFeatureSelectorTransformer - end (192361, 20)\n",
      "transform - ImputerTransformer - begin (192361, 20)\n",
      "transform - ImputerTransformer - end (192361, 20)\n",
      "transform - DateTransformer (192361, 20)\n",
      "transform - DateTransformer - end (192361, 22)\n",
      "transform - RobustScalerTransformer - begin (192361, 22)\n",
      "transform - RobustScalerTransformer - end (192361, 22)\n",
      "Accuracy: (0.6957803296926092,) | Precision:0.4710833721917333 | Recall:0.7339381182124695 | F1-score:0.5738421205942325 | FP:44237 | FN:14283 | VP:39400 | VN:94441\n",
      "over sampled training:\n",
      "Accuracy: (0.706929000995111,) | Precision:0.697079164062661 | Recall:0.7319185451189085 | F1-score:0.7140741574535593 | FP:44108 | FN:37177 | VP:101501 | VN:94570\n",
      "test:\n",
      "Accuracy: (0.6929778960720301,) | Precision:0.46754048534929005 | Recall:0.7348111658456487 | F1-score:0.5714700333768684 | FP:11212 | FN:3553 | VP:9845 | VN:23481\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "\n",
    "print(data.shape[0])\n",
    "data = data.dropna(subset=['Vai Chover Amanha'])\n",
    "#data = data.dropna()\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "print(data.shape[0])\n",
    "\n",
    "X = data.drop('Vai Chover Amanha', axis=1)\n",
    "y = data['Vai Chover Amanha']\n",
    "\n",
    "raw_X_train, X_test, raw_y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=60)\n",
    "\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('manual',ManualFeatureSelectorTransformer()),\n",
    "    ('imputer',ImputerTransformer()),\n",
    "    ('date', DateTransformer()),\n",
    "    #('oht', OneHotEncoderTransformer()),\n",
    "    #('outliers', LocalOutlierTransformer()),\n",
    "    ('scaler', RobustScalerTransformer()),\n",
    "    #('PCA', PCA(n_components=18)),\n",
    "    #('over_sampler', RandomOverSampler(random_state=42)),\n",
    "    #('model', bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight='balanced', splitter='best', max_depth=15, criterion='gini', random_state=42), n_estimators=20, random_state=42)),\n",
    "])\n",
    "\n",
    "X_train = pipeline.fit_transform(handleOutliersZIndex(raw_X_train.copy(),4), raw_y_train.copy())\n",
    "y_train = raw_y_train.copy()\n",
    "\n",
    "X_test = pipeline.transform(X_test.copy())\n",
    "\n",
    "os = RandomOverSampler(random_state=42)\n",
    "X_train, y_train = os.fit_resample(X_train.copy(), y_train.copy())\n",
    "\n",
    "scikit_model = get_bagging(X_train.copy(), y_train.copy())\n",
    "print(\"bagging\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())\n",
    "\n",
    "scikit_model = get_rf(X_train.copy(), y_train.copy())\n",
    "print(\"rf\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())\n",
    "\n",
    "scikit_model = get_ada(X_train, y_train)\n",
    "print(\"ada\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())\n",
    "\n",
    "scikit_model = get_nb(X_train, y_train)\n",
    "print(\"nb\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())\n",
    "\n",
    "scikit_model = get_mlp(X_train, y_train)\n",
    "print(\"mlp\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "\n",
    "data.drop('Radiacao Global', axis=1, inplace=True)\n",
    "data.drop('Cidade', axis=1, inplace=True)\n",
    "data.drop('Codigo', axis=1, inplace=True)\n",
    "data.drop('Latitude', axis=1, inplace=True)\n",
    "data.drop('Longitude', axis=1, inplace=True)\n",
    "\n",
    "data['Data'] = pd.to_datetime(data['Data'])\n",
    "data['Ano'] = data['Data'].dt.year\n",
    "data.drop('Data', axis=1, inplace=True)\n",
    "\n",
    "counter = 0\n",
    "for i in range(2001,2024):\n",
    "    filtro = data.loc[(data['Ano']  == i)].drop('Ano', axis=1)\n",
    "    counter += filtro.isna().any(axis=1).sum()\n",
    "    print('ano-',i,':', filtro.isna().any(axis=1).sum())\n",
    "print('any:',counter)\n",
    "\n",
    "counter = 0\n",
    "for i in range(2001,2024):\n",
    "    filtro = data.loc[(data['Ano']  == i)].drop('Ano', axis=1)\n",
    "    counter += filtro.isna().all(axis=1).sum()\n",
    "    print('ano-',i,':', filtro.isna().all(axis=1).sum())\n",
    "print('all:',counter)\n",
    "\n",
    "counter = 0\n",
    "for i in range(2001,2024):\n",
    "    counter += data.loc[(data['Ano']  == i)]['Vai Chover Amanha'].isna().sum()\n",
    "    print('ano-',i,':', data.loc[(data['Ano']  == i)]['Vai Chover Amanha'].isna().sum())\n",
    "print('y:',counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pressao Maxima 1305\n",
      "Pressao Minima 1282\n",
      "Temperatura Maxima 292\n",
      "Temperatura Minima 1328\n",
      "Temperatura Orvalho Maxima 2972\n",
      "Temperatura Orvalho Minima 1498\n",
      "Umidade Minima 0\n",
      "Umidade Maxima 12532\n",
      "Precipitacao Total 42323\n",
      "Pressao Media 1453\n",
      "Temperatura Media 659\n",
      "Temperatura Orvalho Media 2221\n",
      "Umidade Media 1729\n",
      "Direcao Vento 1787\n",
      "Rajada Maxima de Vento 7208\n",
      "Vento Velocidade Media 5729\n",
      "Radiacao Global 185\n",
      "84503\n",
      "Pressao Maxima 0\n",
      "Pressao Minima 0\n",
      "Temperatura Maxima 0\n",
      "Temperatura Minima 2\n",
      "Temperatura Orvalho Maxima 24\n",
      "Temperatura Orvalho Minima 23\n",
      "Umidade Minima 0\n",
      "Umidade Maxima 1328\n",
      "Precipitacao Total 3107\n",
      "Pressao Media 0\n",
      "Temperatura Media 0\n",
      "Temperatura Orvalho Media 7\n",
      "Umidade Media 42\n",
      "Direcao Vento 0\n",
      "Rajada Maxima de Vento 616\n",
      "Vento Velocidade Media 615\n",
      "Radiacao Global 69\n",
      "5833\n",
      "Precipitacao Total 8528\n",
      "Precipitacao Total 942\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "\n",
    "data = data.dropna()\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "columns = ['Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima',\n",
    "                'Umidade Minima', 'Umidade Maxima','Precipitacao Total', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media', 'Umidade Media', \n",
    "                'Direcao Vento', 'Rajada Maxima de Vento', 'Vento Velocidade Media', 'Radiacao Global']\n",
    "\n",
    "counter = 0\n",
    "for column in columns:\n",
    "    q25, q75 = np.percentile(data[column], 25), np.percentile(data[column], 75)\n",
    "    iqr = q75 - q25\n",
    "    cut_off = iqr * 1.5\n",
    "    lower_bound, upper_bound = q25 - cut_off, q75 + cut_off\n",
    "    outliers = [x for x in data[column] if x < lower_bound or x > upper_bound]\n",
    "    num_outliers = len(outliers)\n",
    "    print(column,num_outliers)\n",
    "    counter += num_outliers\n",
    "print(counter)\n",
    "\n",
    "counter = 0\n",
    "for column in columns:\n",
    "    z_scores = np.abs(stats.zscore(data[column]))\n",
    "    threshold = 4\n",
    "    outliers_mask = (z_scores > threshold)\n",
    "\n",
    "    num_outliers = outliers_mask.sum()\n",
    "    print(column,num_outliers)\n",
    "    counter += outliers_mask.sum()\n",
    "print(counter)\n",
    "\n",
    "column = 'Precipitacao Total'\n",
    "data = data[data[column] != 0]\n",
    "counter = 0\n",
    "q25, q75 = np.percentile(data[column], 25), np.percentile(data[column], 75)\n",
    "iqr = q75 - q25\n",
    "cut_off = iqr * 1.5\n",
    "lower_bound, upper_bound = q25 - cut_off, q75 + cut_off\n",
    "outliers = [x for x in data[column] if x < lower_bound or x > upper_bound]\n",
    "num_outliers = len(outliers)\n",
    "print(column,num_outliers)\n",
    "\n",
    "z_scores = np.abs(stats.zscore(data[column]))\n",
    "threshold = 4\n",
    "outliers_mask = (z_scores > threshold)\n",
    "\n",
    "num_outliers = outliers_mask.sum()\n",
    "print(column,num_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('Radiacao Global', axis=1, inplace=True)\n",
    "data.drop('Cidade', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape[0])\n",
    "data = data.dropna()\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "print(data.shape[0])\n",
    "print(data.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Data'] = pd.to_datetime(data['Data'])\n",
    "data['Ano'] = data['Data'].dt.year\n",
    "data['Mes'] = data['Data'].dt.month\n",
    "data['Dia'] = data['Data'].dt.day\n",
    "data.drop('Data', axis=1, inplace=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoded_data = encoder.fit_transform(data[['Codigo']])\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(['Codigo']))\n",
    "data = pd.concat([data.drop(['Codigo'], axis=1), encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "data[['Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima','Umidade Minima',\n",
    "      'Umidade Maxima','Precipitacao Total', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media', 'Umidade Media', 'Direcao Vento',\n",
    "      'Rajada Maxima de Vento','Vento Velocidade Media', 'Latitude', 'Longitude', 'Ano', 'Mes', 'Dia']] = scaler.fit_transform(\n",
    "          data[['Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima',\n",
    "                'Umidade Minima', 'Umidade Maxima','Precipitacao Total', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media', 'Umidade Media', \n",
    "                'Direcao Vento', 'Rajada Maxima de Vento', 'Vento Velocidade Media', 'Latitude', 'Longitude', 'Ano', 'Mes', 'Dia']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Vai Chover Amanha', axis=1)\n",
    "y = data['Vai Chover Amanha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof = LocalOutlierFactor(contamination=0.1)\n",
    "yhat = lof.fit_predict(X)\n",
    "mask = yhat != -1\n",
    "X = X[mask]\n",
    "y = y[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_X_train, X_test, raw_y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat([raw_X_train, raw_y_train], axis=1)\n",
    "\n",
    "majority_class_data = train_data[train_data['Vai Chover Amanha'] == 'Nao']\n",
    "minority_class_data = train_data[train_data['Vai Chover Amanha'] == 'Sim']\n",
    "upsampled_miNaority_class = resample(minority_class_data, replace=True, n_samples=len(majority_class_data))\n",
    "train_data = pd.concat([majority_class_data, upsampled_miNaority_class], axis=0)\n",
    "\n",
    "balanced_X_train = train_data.drop('Vai Chover Amanha', axis=1)\n",
    "balanced_y_train = train_data['Vai Chover Amanha']\n",
    "\n",
    "print(train_data['Vai Chover Amanha'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42,k_neighbors=15)\n",
    "print(raw_y_train.value_counts())\n",
    "balanced_X_train, balanced_y_train = sm.fit_resample(raw_X_train, raw_y_train)\n",
    "print(balanced_y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = BorderlineSMOTE(random_state=42)\n",
    "print(raw_y_train.value_counts())\n",
    "balanced_X_train, balanced_y_train = sm.fit_resample(raw_X_train, raw_y_train)\n",
    "print(balanced_y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SVMSMOTE(random_state=42)\n",
    "print(raw_y_train.value_counts())\n",
    "balanced_X_train, balanced_y_train = sm.fit_resample(raw_X_train, raw_y_train)\n",
    "print(balanced_y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = KMeansSMOTE(random_state=42)\n",
    "print(raw_y_train.value_counts())\n",
    "balanced_X_train, balanced_y_train = sm.fit_resample(raw_X_train, raw_y_train)\n",
    "\n",
    "print(balanced_y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(balanced_X_train.shape)\n",
    "selector = SelectKBest(k=20)\n",
    "processed_X_train = selector.fit_transform(balanced_X_train, balanced_y_train)\n",
    "processed_y_train = balanced_y_train.copy()\n",
    "print(processed_X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(balanced_X_train.shape)\n",
    "dt = DecisionTreeClassifier(splitter='best', criterion='gini')\n",
    "selector = SequentialFeatureSelector(dt, n_features_to_select=20)\n",
    "processed_X_train = selector.fit_transform(balanced_X_train, balanced_y_train)\n",
    "processed_y_train = balanced_y_train.copy()\n",
    "print(processed_X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(balanced_X_train.shape)\n",
    "rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "selector = SelectFromModel(rf)\n",
    "processed_X_train = selector.fit_transform(balanced_X_train, balanced_y_train)\n",
    "processed_y_train = balanced_y_train.copy()\n",
    "print(processed_X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(balanced_X_train.shape)\n",
    "selector = SelectKBest(k=66)\n",
    "processed_X_train = selector.fit_transform(balanced_X_train, balanced_y_train)\n",
    "processed_y_train = balanced_y_train.copy()\n",
    "print(processed_X_train.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
