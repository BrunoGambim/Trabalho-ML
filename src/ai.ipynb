{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, PowerTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "#from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn import FunctionSampler\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import optuna\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB, ComplementNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, KMeansSMOTE, SVMSMOTE, ADASYN, RandomOverSampler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2, SelectPercentile, SelectFromModel, SequentialFeatureSelector, SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "from imblearn.base import SamplerMixin\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualFeatureSelectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        #print(\"fit - ManualFeatureSelectorTransformer - begin\", X.shape)\n",
    "        #print(\"fit - ManualFeatureSelectorTransformer - end\", X.shape)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #print(\"transform - ManualFeatureSelectorTransformer\", X.shape)\n",
    "        X.drop('Cidade', axis=1, inplace=True)\n",
    "        X.drop('Codigo', axis=1, inplace=True)\n",
    "        #print(\"transform - ManualFeatureSelectorTransformer - end\", X.shape)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImputerTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.imputer_numeric = SimpleImputer(strategy='mean')\n",
    "        self.imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        #print(\"fit - ImputerTransformer - begin\", X.shape)\n",
    "        numeric_cols = X.select_dtypes(include=['number']).columns\n",
    "        categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "        self.imputer_numeric.fit(X[numeric_cols])\n",
    "        self.imputer_categorical.fit(X[categorical_cols])\n",
    "        #print(\"fit - ImputerTransformer - end\", X.shape)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #print(\"transform - ImputerTransformer - begin\", X.shape)\n",
    "        numeric_cols = X.select_dtypes(include=['number']).columns\n",
    "        categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "        X[numeric_cols] = self.imputer_numeric.transform(X[numeric_cols])\n",
    "        X[categorical_cols] = self.imputer_categorical.transform(X[categorical_cols])\n",
    "        #print(\"transform - ImputerTransformer - end\", X.shape)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        #print(\"fit - DateTransformer - begin\", X.shape)\n",
    "        #print(\"fit - DateTransformer - end\", X.shape)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #print(\"transform - DateTransformer\", X.shape)\n",
    "        X['Data'] = pd.to_datetime(X['Data'])\n",
    "        X['Ano'] = X['Data'].dt.year\n",
    "        X['Mes'] = X['Data'].dt.month\n",
    "        X['Dia'] = X['Data'].dt.day\n",
    "        X.drop('Data', axis=1, inplace=True)\n",
    "        #print(\"transform - DateTransformer - end\", X.shape)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustScalerTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.rainfall_scaler = RobustScaler(quantile_range=(6.5, 93.5))\n",
    "        self.scaler = RobustScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        #print(\"fit - RobustScalerTransformer - begin\", X.shape)\n",
    "        self.scaler.fit(X[['Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima',\n",
    "                'Umidade Minima', 'Umidade Maxima', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media', 'Umidade Media', \n",
    "                'Direcao Vento 0H','Direcao Vento 6H','Direcao Vento 12H','Direcao Vento 18H', 'Rajada Maxima de Vento 0H', 'Rajada Maxima de Vento 6H',\n",
    "                'Rajada Maxima de Vento 12H', 'Rajada Maxima de Vento 18H', 'Vento Velocidade Media 0H', 'Vento Velocidade Media 6H',\n",
    "                'Vento Velocidade Media 12H', 'Vento Velocidade Media 18H', 'Latitude', 'Longitude', 'Ano', 'Mes', 'Dia','Radiacao Global', 'Altitude']])\n",
    "        self.rainfall_scaler.fit(X[['Precipitacao Total']])\n",
    "        #print(\"fit - RobustScalerTransformer - end\", X.shape)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #print(\"transform - RobustScalerTransformer - begin\", X.shape)\n",
    "        X[['Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima','Umidade Minima',\n",
    "      'Umidade Maxima', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media', 'Umidade Media', 'Direcao Vento 0H','Direcao Vento 6H','Direcao Vento 12H', \n",
    "      'Direcao Vento 18H', 'Rajada Maxima de Vento 0H', 'Rajada Maxima de Vento 6H', 'Rajada Maxima de Vento 12H', 'Rajada Maxima de Vento 18H',\n",
    "      'Vento Velocidade Media 0H', 'Vento Velocidade Media 6H', 'Vento Velocidade Media 12H', 'Vento Velocidade Media 18H', 'Latitude', 'Longitude', 'Ano', 'Mes', 'Dia',\n",
    "      'Radiacao Global', 'Altitude']] = self.scaler.transform(\n",
    "          X[['Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima',\n",
    "                'Umidade Minima', 'Umidade Maxima', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media', 'Umidade Media', 'Direcao Vento 0H', \n",
    "                'Direcao Vento 6H','Direcao Vento 12H','Direcao Vento 18H', 'Rajada Maxima de Vento 0H', 'Rajada Maxima de Vento 6H', 'Rajada Maxima de Vento 12H',\n",
    "                'Rajada Maxima de Vento 18H', 'Vento Velocidade Media 0H', 'Vento Velocidade Media 6H', 'Vento Velocidade Media 12H', 'Vento Velocidade Media 18H',\n",
    "                'Latitude', 'Longitude', 'Ano', 'Mes', 'Dia', 'Radiacao Global', 'Altitude']])\n",
    "        X['Precipitacao Total'] = self.rainfall_scaler.transform(X[['Precipitacao Total']])\n",
    "        #print(\"transform - RobustScalerTransformer - end\", X.shape)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRobustScalerTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.rainfall_scaler = RobustScaler()\n",
    "        self.scaler = RobustScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        #print(\"fit - RobustScalerTransformer - begin\", X.shape)\n",
    "        self.scaler.fit(X[['Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima',\n",
    "                'Umidade Minima', 'Umidade Maxima', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media', 'Umidade Media', \n",
    "                'Direcao Vento 0H','Direcao Vento 6H','Direcao Vento 12H','Direcao Vento 18H', 'Rajada Maxima de Vento 0H', 'Rajada Maxima de Vento 6H',\n",
    "                'Rajada Maxima de Vento 12H', 'Rajada Maxima de Vento 18H', 'Vento Velocidade Media 0H', 'Vento Velocidade Media 6H',\n",
    "                'Vento Velocidade Media 12H', 'Vento Velocidade Media 18H', 'Latitude', 'Longitude', 'Ano', 'Mes', 'Dia','Radiacao Global', 'Altitude']])\n",
    "        self.rainfall_scaler.fit(X[['Precipitacao Total']])\n",
    "        #print(\"fit - RobustScalerTransformer - end\", X.shape)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #print(\"transform - RobustScalerTransformer - begin\", X.shape)\n",
    "        X[['Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima','Umidade Minima',\n",
    "      'Umidade Maxima', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media', 'Umidade Media', 'Direcao Vento 0H','Direcao Vento 6H','Direcao Vento 12H', \n",
    "      'Direcao Vento 18H', 'Rajada Maxima de Vento 0H', 'Rajada Maxima de Vento 6H', 'Rajada Maxima de Vento 12H', 'Rajada Maxima de Vento 18H',\n",
    "      'Vento Velocidade Media 0H', 'Vento Velocidade Media 6H', 'Vento Velocidade Media 12H', 'Vento Velocidade Media 18H', 'Latitude', 'Longitude', 'Ano', 'Mes', 'Dia',\n",
    "      'Radiacao Global', 'Altitude']] = self.scaler.transform(\n",
    "          X[['Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima',\n",
    "                'Umidade Minima', 'Umidade Maxima', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media', 'Umidade Media', 'Direcao Vento 0H', \n",
    "                'Direcao Vento 6H','Direcao Vento 12H','Direcao Vento 18H', 'Rajada Maxima de Vento 0H', 'Rajada Maxima de Vento 6H', 'Rajada Maxima de Vento 12H',\n",
    "                'Rajada Maxima de Vento 18H', 'Vento Velocidade Media 0H', 'Vento Velocidade Media 6H', 'Vento Velocidade Media 12H', 'Vento Velocidade Media 18H',\n",
    "                'Latitude', 'Longitude', 'Ano', 'Mes', 'Dia', 'Radiacao Global', 'Altitude']])\n",
    "        X['Precipitacao Total'] = self.rainfall_scaler.transform(X[['Precipitacao Total']])\n",
    "        #print(\"transform - RobustScalerTransformer - end\", X.shape)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handleOutliersZIndex(X, threshold = 3):\n",
    "    #print(\"handleOutliersZIndex - begin\", X.shape)\n",
    "    columns = ['Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima',\n",
    "                'Umidade Minima', 'Umidade Maxima', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media', 'Umidade Media', \n",
    "                'Direcao Vento 0H','Direcao Vento 6H','Direcao Vento 12H','Direcao Vento 18H', 'Rajada Maxima de Vento 0H', 'Rajada Maxima de Vento 6H', \n",
    "                'Rajada Maxima de Vento 12H', 'Rajada Maxima de Vento 18H', 'Vento Velocidade Media 0H', 'Vento Velocidade Media 6H', 'Vento Velocidade Media 12H', \n",
    "                'Vento Velocidade Media 18H', 'Radiacao Global', 'Altitude']\n",
    "    \n",
    "    for column in columns:\n",
    "        non_zero_mask = (X[column].notna())\n",
    "\n",
    "        nz_X = X[non_zero_mask]\n",
    "        z_scores = np.abs(stats.zscore(nz_X[column]))\n",
    "        outliers_mask = (z_scores > threshold)\n",
    "\n",
    "        nz_X.loc[outliers_mask, column] = nz_X[(~outliers_mask)][column].mean()\n",
    "\n",
    "        X.loc[non_zero_mask, column] = nz_X[column]\n",
    "    \n",
    "    column = 'Precipitacao Total'\n",
    "    non_zero_mask = (X[column] != 0) & (X[column].notna())\n",
    "\n",
    "    nz_X = X[non_zero_mask]\n",
    "    z_scores = np.abs(stats.zscore(nz_X[column]))\n",
    "    outliers_mask = (z_scores > threshold)\n",
    "\n",
    "    nz_X.loc[outliers_mask, column] = nz_X[(~outliers_mask)][column].mean()\n",
    "\n",
    "    X.loc[non_zero_mask, column] = nz_X[column]\n",
    "\n",
    "    #print(\"handleOutliersZIndex - end\", X.shape)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handleOutliersSimpleZIndex(X, threshold = 3):\n",
    "    #print(\"handleOutliersZIndex - begin\", X.shape)\n",
    "    columns = ['Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima',\n",
    "                'Umidade Minima', 'Umidade Maxima', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media', 'Umidade Media', \n",
    "                'Direcao Vento 0H','Direcao Vento 6H','Direcao Vento 12H','Direcao Vento 18H', 'Rajada Maxima de Vento 0H', 'Rajada Maxima de Vento 6H', \n",
    "                'Rajada Maxima de Vento 12H', 'Rajada Maxima de Vento 18H', 'Vento Velocidade Media 0H', 'Vento Velocidade Media 6H', 'Vento Velocidade Media 12H', \n",
    "                'Vento Velocidade Media 18H', 'Radiacao Global', 'Altitude', 'Precipitacao Total']\n",
    "    \n",
    "    for column in columns:\n",
    "        non_zero_mask = (X[column].notna())\n",
    "\n",
    "        nz_X = X[non_zero_mask]\n",
    "        z_scores = np.abs(stats.zscore(nz_X[column]))\n",
    "        outliers_mask = (z_scores > threshold)\n",
    "\n",
    "        nz_X.loc[outliers_mask, column] = nz_X[(~outliers_mask)][column].mean()\n",
    "\n",
    "        X.loc[non_zero_mask, column] = nz_X[column]\n",
    "\n",
    "    #print(\"handleOutliersZIndex - end\", X.shape)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlp(data_x, data_y):\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(10,20,20,10), activation='relu', max_iter=250, random_state=42)\n",
    "    mlp.fit(data_x, data_y)\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dt(data_x, data_y):\n",
    "    dt = DecisionTreeClassifier(class_weight='balanced', splitter='best', max_depth=None, criterion='gini')\n",
    "    dt.fit(data_x, data_y)\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rf(data_x, data_y):\n",
    "    rf = RandomForestClassifier(class_weight='balanced', max_depth=15, n_estimators=100, random_state=42)\n",
    "    rf.fit(data_x, data_y)\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sgd(data_x, data_y):\n",
    "    sgd = SGDClassifier(max_iter=3000, tol=1e-3, class_weight='balanced', random_state=42)\n",
    "    sgd.fit(data_x, data_y)\n",
    "    return sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logistic_regression(data_x, data_y):\n",
    "    lr = LogisticRegression(class_weight='balanced', max_iter=5000, random_state=42)\n",
    "    lr.fit(data_x, data_y)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bagging(data_x, data_y):\n",
    "   base_classifier = DecisionTreeClassifier(class_weight='balanced', splitter='best', max_depth=15, criterion='gini', random_state=42)\n",
    "   bagging = BaggingClassifier(base_estimator=base_classifier, n_estimators=20, random_state=42)\n",
    "   bagging.fit(data_x, data_y)\n",
    "   return bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_svc(data_x, data_y):\n",
    "    linear_svc = LinearSVC(class_weight='balanced', random_state=42)\n",
    "    linear_svc.fit(data_x, data_y)\n",
    "    return linear_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_knn(data_x, data_y):\n",
    "    knn = KNeighborsClassifier(n_neighbors=20)\n",
    "    knn.fit(data_x, data_y)\n",
    "    return knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nb(data_x, data_y):\n",
    "    nb = BernoulliNB()\n",
    "    nb.fit(data_x, data_y)\n",
    "    return nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ada(data_x, data_y):\n",
    "    dt = DecisionTreeClassifier(class_weight='balanced', splitter='best', max_depth=5, criterion='gini')\n",
    "    ada = AdaBoostClassifier(base_estimator=dt, n_estimators=100, algorithm=\"SAMME\", random_state=42)\n",
    "    ada.fit(data_x, data_y)\n",
    "    return ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble(data_x, data_y):\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(10,20,20,10), activation='relu', max_iter=15, random_state=42)\n",
    "    rf = RandomForestClassifier(class_weight='balanced', max_depth=15, n_estimators=20, random_state=42)\n",
    "    base_classifier = DecisionTreeClassifier(class_weight='balanced', splitter='best', max_depth=15, criterion='gini', random_state=42)\n",
    "    bagging = BaggingClassifier(base_estimator=base_classifier, n_estimators=20, random_state=42)\n",
    "    ensemble = VotingClassifier(estimators=[('mlp', mlp), ('rf', rf), ('bagging', bagging)], voting='hard')\n",
    "    ensemble.fit(data_x, data_y)\n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lda(data_x, data_y):\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    lda.fit(data_x, data_y)\n",
    "    return lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result_scikit(model, data_x, data_y):\n",
    "    y_pred = model.predict(data_x)\n",
    "    accuracy = accuracy_score(data_y, y_pred),\n",
    "    y_true  = (data_y == 'Sim').astype(int)\n",
    "    y_pred  = (y_pred == 'Sim').astype(int)\n",
    "\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"Accuracy: {str(accuracy).replace('.',',')} | Precision:{str(precision).replace('.',',')} | Recall:{str(recall).replace('.',',')} | F1-score:{str(f1).replace('.',',')}\")\n",
    "    print(f\"{str(accuracy).replace('.',',')[1:-2]}\\t{str(precision).replace('.',',')}\\t{str(recall).replace('.',',')}\\t{str(f1).replace('.',',')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "\n",
    "print(data.shape[0])\n",
    "data = data.dropna(subset=['Vai Chover Amanha'])#Removendo linhas com valores faltantes na coluna de outliers\n",
    "\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "print(data.shape[0])\n",
    "\n",
    "X = data.drop('Vai Chover Amanha', axis=1)\n",
    "y = data['Vai Chover Amanha']\n",
    "\n",
    "raw_X_train, X_test, raw_y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42) #Holdout 15%/15%/70%\n",
    "raw_X_train_2, X_test_2, raw_y_train_2, y_test_2 = train_test_split(raw_X_train, raw_y_train, test_size=0.177, random_state=42)\n",
    "\n",
    "selector = SelectKBest(k=28) #Seleção de features com o SelectKBest\n",
    "#rf = RandomForestClassifier(class_weight='balanced', max_depth=20, n_estimators=100, random_state=42)\n",
    "#selector = SelectFromModel(rf) #Seleção de features com o SelectFromModel\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('manual',ManualFeatureSelectorTransformer()),#Remoção de colunas de código e cidades\n",
    "    ('imputer',ImputerTransformer()),#Lidando com valores faltantes restantes\n",
    "    ('date', DateTransformer()),#Separa coluna de data em dia, mês e ano\n",
    "    ('scaler', RobustScalerTransformer()),#Normalização dos dados\n",
    "    #('scaler', SimpleRobustScalerTransformer()),#Normalização dos dados\n",
    "    #('PCA', PCA(n_components=21)),\n",
    "    ('selector', selector),\n",
    "    ])\n",
    "\n",
    "#X_train = pipeline.fit_transform(handleOutliersSimpleZIndex(raw_X_train_2.copy(),3), raw_y_train_2.copy()) # Abordagem simples para lidar com outliers\n",
    "#X_train = pipeline.fit_transform(handleOutliersZIndex(raw_X_train_2.copy(),3), raw_y_train_2.copy()) # Abordagem complexa para lidar com outliers\n",
    "\n",
    "X_train = pipeline.fit_transform(raw_X_train_2.copy(), raw_y_train_2.copy())\n",
    "y_train = raw_y_train_2.copy()\n",
    "\n",
    "X_test_2 = pipeline.transform(X_test_2.copy())\n",
    "\n",
    "# Abordagens para lidar com dados desbalanceados\n",
    "os = RandomOverSampler(random_state=42)\n",
    "#os = SMOTE(random_state=42, k_neighbors=10)\n",
    "#os = BorderlineSMOTE(random_state=42, k_neighbors=10)\n",
    "X_train, y_train = os.fit_resample(X_train.copy(), y_train.copy())\n",
    "\n",
    "scikit_model = get_rf(X_train.copy(), y_train.copy())\n",
    "print(\"rf\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train_2.copy()), raw_y_train_2.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test_2.copy(),y_test_2.copy())\n",
    "\n",
    "scikit_model = get_ada(X_train, y_train)\n",
    "print(\"ada\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train_2.copy()), raw_y_train_2.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test_2.copy(),y_test_2.copy())\n",
    "\n",
    "scikit_model = get_logistic_regression(X_train, y_train)\n",
    "print(\"logistic_regression\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train_2.copy()), raw_y_train_2.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test_2.copy(),y_test_2.copy())\n",
    "\n",
    "scikit_model = get_mlp(X_train, y_train)\n",
    "print(\"mlp\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train_2.copy()), raw_y_train_2.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test_2.copy(),y_test_2.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "\n",
    "print(data.shape[0])\n",
    "data = data.dropna(subset=['Vai Chover Amanha'])#Removendo linhas com valores faltantes na coluna de outliers\n",
    "\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "print(data.shape[0])\n",
    "\n",
    "X = data.drop('Vai Chover Amanha', axis=1)\n",
    "y = data['Vai Chover Amanha']\n",
    "\n",
    "raw_X_train, X_test, raw_y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42) #Holdout 15%/15%/70%\n",
    "raw_X_train_2, X_test_2, raw_y_train_2, y_test_2 = train_test_split(raw_X_train, raw_y_train, test_size=0.177, random_state=42)\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('manual',ManualFeatureSelectorTransformer()),#Remoção de colunas de código e cidades\n",
    "    ('imputer',ImputerTransformer()),#Lidando com valores faltantes restantes\n",
    "    ('date', DateTransformer()),#Separa coluna de data em dia, mês e ano\n",
    "    ('scaler', RobustScalerTransformer()),#Normalização dos dados\n",
    "    ])\n",
    "\n",
    "X_train = pipeline.fit_transform(raw_X_train.copy(), raw_y_train.copy())\n",
    "y_train = raw_y_train.copy()\n",
    "\n",
    "X_test = pipeline.transform(X_test.copy())\n",
    "\n",
    "# Abordagens para lidar com dados desbalanceados\n",
    "os = RandomOverSampler(random_state=42)\n",
    "X_train, y_train = os.fit_resample(X_train.copy(), y_train.copy())\n",
    "\n",
    "scikit_model = get_lda(X_train.copy(), y_train.copy())\n",
    "print(\"lda\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())\n",
    "\n",
    "scikit_model = get_bagging(X_train.copy(), y_train.copy())\n",
    "print(\"bagging\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())\n",
    "\n",
    "scikit_model = get_rf(X_train.copy(), y_train.copy())\n",
    "print(\"rf\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())\n",
    "\n",
    "scikit_model = get_ada(X_train, y_train)\n",
    "print(\"ada\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())\n",
    "\n",
    "scikit_model = get_logistic_regression(X_train, y_train)\n",
    "print(\"logistic_regression\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())\n",
    "\n",
    "scikit_model = get_mlp(X_train, y_train)\n",
    "print(\"mlp\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())\n",
    "\n",
    "scikit_model = get_ensemble(X_train, y_train)\n",
    "print(\"ensemble\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())\n",
    "\n",
    "scikit_model = get_sgd(X_train, y_train)\n",
    "print(\"sgd\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())\n",
    "\n",
    "scikit_model = get_linear_svc(X_train, y_train)\n",
    "print(\"svc\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())\n",
    "\n",
    "scikit_model = get_nb(X_train, y_train)\n",
    "print(\"nb\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())\n",
    "\n",
    "scikit_model = get_knn(X_train, y_train)\n",
    "print(\"knn\")\n",
    "print(\"training:\")\n",
    "plot_result_scikit(scikit_model, pipeline.transform(raw_X_train.copy()), raw_y_train.copy()) \n",
    "print(\"over sampled training:\")\n",
    "plot_result_scikit(scikit_model, X_train.copy(), y_train.copy()) \n",
    "print(\"test:\")\n",
    "plot_result_scikit(scikit_model, X_test.copy(),y_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "\n",
    "print(data.shape[0])\n",
    "data = data.dropna(subset=['Vai Chover Amanha'])#Removendo linhas com valores faltantes na coluna de outliers\n",
    "\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "print(data.shape[0])\n",
    "\n",
    "X = data.drop('Vai Chover Amanha', axis=1)\n",
    "y = data['Vai Chover Amanha']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)\n",
    "\n",
    "    \n",
    "\n",
    "def objective(trial):\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 5)\n",
    "    layers = []\n",
    "    for i in range(n_layers):\n",
    "        layers.append(trial.suggest_int(f'n_units_{i}', 1, 30) * 10)\n",
    "\n",
    "    \n",
    "    activation = trial.suggest_categorical('activation', ['identity', 'logistic', 'tanh', 'relu'])\n",
    "    solver = trial.suggest_categorical('solver', ['sgd', 'adam'])\n",
    "    alpha = trial.suggest_float('alpha', 1e-5, 1e-1, log=True)\n",
    "    learning_rate_init = trial.suggest_float('learning_rate_init', 1e-5, 1e-1, log=True)\n",
    "    max_iter = trial.suggest_int('max_iter', 30, 40)\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('manual',ManualFeatureSelectorTransformer()),\n",
    "        ('imputer',ImputerTransformer()),\n",
    "        ('date', DateTransformer()),\n",
    "        ('scaler', RobustScalerTransformer()),\n",
    "        ('os', RandomOverSampler(random_state=42)),\n",
    "        ('model', MLPClassifier(hidden_layer_sizes=tuple(layers), activation=activation, solver=solver, max_iter=max_iter*10, alpha=alpha, learning_rate_init=learning_rate_init, random_state=42)),\n",
    "        ])\n",
    "\n",
    "    X_train_2, X_val, y_train_2, y_val = train_test_split(X_train.copy(), y_train.copy(), stratify=y_train, test_size=0.177, random_state=42)\n",
    "    pipeline.fit(X_train_2, y_train_2)\n",
    "\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "    y_true  = (y_val == 'Sim').astype(int)\n",
    "    y_pred  = (y_pred == 'Sim').astype(int)\n",
    "\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"Accuracy: {str(accuracy).replace('.',',')} | Precision:{str(precision).replace('.',',')} | Recall:{str(recall).replace('.',',')} | F1-score:{str(f1).replace('.',',')}\")\n",
    "    #score = cross_val_score(pipeline, X.copy(), y.copy(), n_jobs=-1, cv=5).mean()\n",
    "    return f1\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "for trial in study.trials:\n",
    "    print(f\"Trial {trial.number}:\")\n",
    "    print(f\"  Value: {trial.value}\")\n",
    "    print(\"  Params:\")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n\\n\\nBest trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(f\" Val Value: {trial.value}\")\n",
    "print(\" Val Params:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "layers = []\n",
    "for key, value in trial.params.items():\n",
    "    if key[:7] == 'n_units':\n",
    "        layers.append(value * 10)\n",
    "    \n",
    "print(f\"layers: {layers}\")\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('manual',ManualFeatureSelectorTransformer()),\n",
    "    ('imputer',ImputerTransformer()),\n",
    "    ('date', DateTransformer()),\n",
    "    ('scaler', RobustScalerTransformer()),\n",
    "    ('os', RandomOverSampler(random_state=42)),\n",
    "    ('model', MLPClassifier(hidden_layer_sizes=tuple(layers), activation=trial.params['activation'], solver=trial.params['solver'], max_iter=trial.params['max_iter']*10, alpha=trial.params['alpha'], learning_rate_init=trial.params['learning_rate_init'], random_state=42)),\n",
    "    ])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "y_true  = (y_test == 'Sim').astype(int)\n",
    "y_pred  = (y_pred == 'Sim').astype(int)\n",
    "\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Test: Accuracy: {str(accuracy).replace('.',',')} | Precision:{str(precision).replace('.',',')} | Recall:{str(recall).replace('.',',')} | F1-score:{str(f1).replace('.',',')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "data = data.dropna(subset=['Vai Chover Amanha'])#Removendo linhas com valores faltantes na coluna de outliers\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X = data.drop('Vai Chover Amanha', axis=1)\n",
    "y = data['Vai Chover Amanha']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('manual',ManualFeatureSelectorTransformer()),\n",
    "    ('imputer',ImputerTransformer()),\n",
    "    ('date', DateTransformer()),\n",
    "    ('scaler', RobustScalerTransformer()),\n",
    "    ('os', RandomOverSampler(random_state=42)),\n",
    "    ('model', MLPClassifier(hidden_layer_sizes=(40,20,300), activation='tanh', solver='adam', max_iter=350, alpha=0.00026300311127326896, learning_rate_init=0.00015373773782206807, random_state=42)),\n",
    "    ])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "for i in range(2001,2025):\n",
    "    print(i)\n",
    "    f_X_test = X_test.copy().loc[pd.to_datetime(X_test['Data']).dt.year == i]\n",
    "    f_X_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    f_y_test = y_test.copy().loc[pd.to_datetime(X_test['Data']).dt.year == i]\n",
    "    f_y_test.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "\n",
    "    y_pred = pipeline.predict(f_X_test)\n",
    "    accuracy = accuracy_score(f_y_test, y_pred)\n",
    "\n",
    "    y_true  = (f_y_test == 'Sim').astype(int)\n",
    "    y_pred  = (y_pred == 'Sim').astype(int)\n",
    "\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "    print(f\"Test: Accuracy: {str(accuracy).replace('.',',')} | Precision:{str(precision).replace('.',',')} | Recall:{str(recall).replace('.',',')} | F1-score:{str(f1).replace('.',',')}\")\n",
    "    print(f\"{str(accuracy).replace('.',',')[1:-2]}\\t{str(precision).replace('.',',')}\\t{str(recall).replace('.',',')}\\t{str(f1).replace('.',',')}\")\n",
    "\n",
    "for cidade in data['Cidade'].unique():\n",
    "    print(cidade)\n",
    "    f_X_test = X_test.copy().loc[X_test['Cidade'] == cidade]\n",
    "    f_X_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    f_y_test = y_test.copy().loc[X_test['Cidade'] == cidade]\n",
    "    f_y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    y_pred = pipeline.predict(f_X_test)\n",
    "    accuracy = accuracy_score(f_y_test, y_pred)\n",
    "\n",
    "    y_true  = (f_y_test == 'Sim').astype(int)\n",
    "    y_pred  = (y_pred == 'Sim').astype(int)\n",
    "\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "    print(f\"Test: Accuracy: {str(accuracy).replace('.',',')} | Precision:{str(precision).replace('.',',')} | Recall:{str(recall).replace('.',',')} | F1-score:{str(f1).replace('.',',')}\")\n",
    "    print(f\"{str(accuracy).replace('.',',')[1:-2]}\\t{str(precision).replace('.',',')}\\t{str(recall).replace('.',',')}\\t{str(f1).replace('.',',')}\")\n",
    "\n",
    "y_pred = pipeline.predict(X_test.copy())\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "y_true  = (y_test == 'Sim').astype(int)\n",
    "y_pred  = (y_pred == 'Sim').astype(int)\n",
    "\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "FP = np.sum((y_pred == 1) & (y_true == 0))\n",
    "VP = np.sum((y_pred == 1) & (y_true == 1))\n",
    "FN = np.sum((y_pred == 0) & (y_true == 1))\n",
    "VN = np.sum((y_pred == 0) & (y_true == 0))\n",
    "\n",
    "print(f\"VP: {VP} - VN: {VN} - FP: {FP} - FN: {FN}\")\n",
    "print(f\"{VP}\\t{VN}\\t{FP}\\t{FN}\")\n",
    "print(f\"Test: Accuracy: {str(accuracy).replace('.',',')} | Precision:{str(precision).replace('.',',')} | Recall:{str(recall).replace('.',',')} | F1-score:{str(f1).replace('.',',')}\")\n",
    "print(f\"{str(accuracy).replace('.',',')[1:-2]}\\t{str(precision).replace('.',',')}\\t{str(recall).replace('.',',')}\\t{str(f1).replace('.',',')}\")\n",
    "\n",
    "class_labels = pipeline.classes_\n",
    "class_index = np.where(class_labels == \"Sim\")[0][0]\n",
    "\n",
    "y_probs = pipeline.predict_proba(X_test.copy())[:, class_index]\n",
    "\n",
    "y_true  = (y_test.copy() == 'Sim').astype(int)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_true, y_probs)\n",
    "pr_auc = auc(recall, precision)\n",
    "print('auc: ', pr_auc)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, marker='.')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precisão')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "data = data.dropna(subset=['Vai Chover Amanha'])#Removendo linhas com valores faltantes na coluna de outliers\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X = data.drop('Vai Chover Amanha', axis=1)\n",
    "y = data['Vai Chover Amanha']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('manual',ManualFeatureSelectorTransformer()),\n",
    "    ('imputer',ImputerTransformer()),\n",
    "    ('date', DateTransformer()),\n",
    "    ('scaler', RobustScalerTransformer()),\n",
    "    ('os', RandomOverSampler(random_state=42)),\n",
    "    ('model', MLPClassifier(hidden_layer_sizes=(100,70,270,160,280), activation='logistic', solver='adam', max_iter=310, alpha=0.0003629543804893746, learning_rate_init=0.00044484921922101567, random_state=42)),\n",
    "    ])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "for i in range(2001,2025):\n",
    "    print(i)\n",
    "    f_X_test = X_test.copy().loc[pd.to_datetime(X_test['Data']).dt.year == i]\n",
    "    f_X_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    f_y_test = y_test.copy().loc[pd.to_datetime(X_test['Data']).dt.year == i]\n",
    "    f_y_test.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "\n",
    "    y_pred = pipeline.predict(f_X_test)\n",
    "    accuracy = accuracy_score(f_y_test, y_pred)\n",
    "\n",
    "    y_true  = (f_y_test == 'Sim').astype(int)\n",
    "    y_pred  = (y_pred == 'Sim').astype(int)\n",
    "\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "    print(f\"Test: Accuracy: {str(accuracy).replace('.',',')} | Precision:{str(precision).replace('.',',')} | Recall:{str(recall).replace('.',',')} | F1-score:{str(f1).replace('.',',')}\")\n",
    "    print(f\"{str(accuracy).replace('.',',')[1:-2]}\\t{str(precision).replace('.',',')}\\t{str(recall).replace('.',',')}\\t{str(f1).replace('.',',')}\")\n",
    "\n",
    "for cidade in data['Cidade'].unique():\n",
    "    print(cidade)\n",
    "    f_X_test = X_test.copy().loc[X_test['Cidade'] == cidade]\n",
    "    f_X_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    f_y_test = y_test.copy().loc[X_test['Cidade'] == cidade]\n",
    "    f_y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    y_pred = pipeline.predict(f_X_test)\n",
    "    accuracy = accuracy_score(f_y_test, y_pred)\n",
    "\n",
    "    y_true  = (f_y_test == 'Sim').astype(int)\n",
    "    y_pred  = (y_pred == 'Sim').astype(int)\n",
    "\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "    print(f\"Test: Accuracy: {str(accuracy).replace('.',',')} | Precision:{str(precision).replace('.',',')} | Recall:{str(recall).replace('.',',')} | F1-score:{str(f1).replace('.',',')}\")\n",
    "    print(f\"{str(accuracy).replace('.',',')[1:-2]}\\t{str(precision).replace('.',',')}\\t{str(recall).replace('.',',')}\\t{str(f1).replace('.',',')}\")\n",
    "\n",
    "y_pred = pipeline.predict(X_test.copy())\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "y_true  = (y_test == 'Sim').astype(int)\n",
    "y_pred  = (y_pred == 'Sim').astype(int)\n",
    "\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "FP = np.sum((y_pred == 1) & (y_true == 0))\n",
    "VP = np.sum((y_pred == 1) & (y_true == 1))\n",
    "FN = np.sum((y_pred == 0) & (y_true == 1))\n",
    "VN = np.sum((y_pred == 0) & (y_true == 0))\n",
    "\n",
    "print(f\"VP: {VP} - VN: {VN} - FP: {FP} - FN: {FN}\")\n",
    "print(f\"{VP}\\t{VN}\\t{FP}\\t{FN}\")\n",
    "print(f\"Test: Accuracy: {str(accuracy).replace('.',',')} | Precision:{str(precision).replace('.',',')} | Recall:{str(recall).replace('.',',')} | F1-score:{str(f1).replace('.',',')}\")\n",
    "print(f\"{str(accuracy).replace('.',',')[1:-2]}\\t{str(precision).replace('.',',')}\\t{str(recall).replace('.',',')}\\t{str(f1).replace('.',',')}\")\n",
    "\n",
    "class_labels = pipeline.classes_\n",
    "class_index = np.where(class_labels == \"Sim\")[0][0]\n",
    "\n",
    "y_probs = pipeline.predict_proba(X_test.copy())[:, class_index]\n",
    "\n",
    "y_true  = (y_test.copy() == 'Sim').astype(int)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_true, y_probs)\n",
    "pr_auc = auc(recall, precision)\n",
    "print('auc: ', pr_auc)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, marker='.')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precisão')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "\n",
    "print(data.shape[0])\n",
    "data = data.dropna(subset=['Vai Chover Amanha'])#Removendo linhas com valores faltantes na coluna de outliers\n",
    "\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "print(data.shape[0])\n",
    "\n",
    "X = data.drop('Vai Chover Amanha', axis=1)\n",
    "y = data['Vai Chover Amanha']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)\n",
    "\n",
    "    \n",
    "\n",
    "def objective(trial):\n",
    "    algorithm = trial.suggest_categorical('algorithm', ['SAMME', 'SAMME.R'])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.25, 2)\n",
    "    n_estimators = trial.suggest_int('n_estimators', 5, 30)\n",
    "    criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 8)\n",
    "\n",
    "    dt = DecisionTreeClassifier(class_weight='balanced', splitter='best', max_depth=max_depth, criterion=criterion, random_state=42)\n",
    "    ada = AdaBoostClassifier(base_estimator=dt, n_estimators=n_estimators*10, algorithm=algorithm, learning_rate=learning_rate, random_state=42)\n",
    "    \n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('manual',ManualFeatureSelectorTransformer()),\n",
    "        ('imputer',ImputerTransformer()),\n",
    "        ('date', DateTransformer()),\n",
    "        ('scaler', RobustScalerTransformer()),\n",
    "        ('os', RandomOverSampler(random_state=42)),\n",
    "        ('model', ada),\n",
    "        ])\n",
    "\n",
    "    X_train_2, X_val, y_train_2, y_val = train_test_split(X_train.copy(), y_train.copy(), stratify=y_train, test_size=0.177, random_state=42)\n",
    "    pipeline.fit(X_train_2, y_train_2)\n",
    "\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "    y_true  = (y_val == 'Sim').astype(int)\n",
    "    y_pred  = (y_pred == 'Sim').astype(int)\n",
    "\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"Accuracy: {str(accuracy).replace('.',',')} | Precision:{str(precision).replace('.',',')} | Recall:{str(recall).replace('.',',')} | F1-score:{str(f1).replace('.',',')}\")\n",
    "    #score = cross_val_score(pipeline, X.copy(), y.copy(), n_jobs=-1, cv=5).mean()\n",
    "    return f1\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "for trial in study.trials:\n",
    "    print(f\"Trial {trial.number}:\")\n",
    "    print(f\"  Value: {trial.value}\")\n",
    "    print(\"  Params:\")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n\\n\\nBest trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(f\" Val Value: {trial.value}\")\n",
    "print(\" Val Params:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "dt = DecisionTreeClassifier(class_weight='balanced', splitter='best', max_depth=trial.params['max_depth'], criterion=trial.params['criterion'], random_state=42)\n",
    "ada = AdaBoostClassifier(base_estimator=dt, n_estimators=trial.params['n_estimators']*10, algorithm=trial.params['algorithm'], learning_rate=trial.params['learning_rate'], random_state=42)\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('manual',ManualFeatureSelectorTransformer()),\n",
    "    ('imputer',ImputerTransformer()),\n",
    "    ('date', DateTransformer()),\n",
    "    ('scaler', RobustScalerTransformer()),\n",
    "    ('os', RandomOverSampler(random_state=42)),\n",
    "    ('model', ada),\n",
    "    ])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "y_true  = (y_test == 'Sim').astype(int)\n",
    "y_pred  = (y_pred == 'Sim').astype(int)\n",
    "\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Test: Accuracy: {str(accuracy).replace('.',',')} | Precision:{str(precision).replace('.',',')} | Recall:{str(recall).replace('.',',')} | F1-score:{str(f1).replace('.',',')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "data = data.dropna(subset=['Vai Chover Amanha'])#Removendo linhas com valores faltantes na coluna de outliers\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X = data.drop('Vai Chover Amanha', axis=1)\n",
    "y = data['Vai Chover Amanha']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)\n",
    "\n",
    "dt = DecisionTreeClassifier(class_weight='balanced', splitter='best', max_depth=8, criterion='gini', random_state=42)\n",
    "ada = AdaBoostClassifier(base_estimator=dt, n_estimators=16*10, algorithm='SAMME', learning_rate=0.645003613327735, random_state=42)\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('manual',ManualFeatureSelectorTransformer()),\n",
    "    ('imputer',ImputerTransformer()),\n",
    "    ('date', DateTransformer()),\n",
    "    ('scaler', RobustScalerTransformer()),\n",
    "    ('os', RandomOverSampler(random_state=42)),\n",
    "    ('model', ada),\n",
    "    ])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "for i in range(2001,2025):\n",
    "    print(i)\n",
    "    f_X_test = X_test.copy().loc[pd.to_datetime(X_test['Data']).dt.year == i]\n",
    "    f_X_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    f_y_test = y_test.copy().loc[pd.to_datetime(X_test['Data']).dt.year == i]\n",
    "    f_y_test.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "\n",
    "    y_pred = pipeline.predict(f_X_test)\n",
    "    accuracy = accuracy_score(f_y_test, y_pred)\n",
    "\n",
    "    y_true  = (f_y_test == 'Sim').astype(int)\n",
    "    y_pred  = (y_pred == 'Sim').astype(int)\n",
    "\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "    print(f\"Test: Accuracy: {str(accuracy).replace('.',',')} | Precision:{str(precision).replace('.',',')} | Recall:{str(recall).replace('.',',')} | F1-score:{str(f1).replace('.',',')}\")\n",
    "    print(f\"{str(accuracy).replace('.',',')[1:-2]}\\t{str(precision).replace('.',',')}\\t{str(recall).replace('.',',')}\\t{str(f1).replace('.',',')}\")\n",
    "\n",
    "for cidade in data['Cidade'].unique():\n",
    "    print(cidade)\n",
    "    f_X_test = X_test.copy().loc[X_test['Cidade'] == cidade]\n",
    "    f_X_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    f_y_test = y_test.copy().loc[X_test['Cidade'] == cidade]\n",
    "    f_y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    y_pred = pipeline.predict(f_X_test)\n",
    "    accuracy = accuracy_score(f_y_test, y_pred)\n",
    "\n",
    "    y_true  = (f_y_test == 'Sim').astype(int)\n",
    "    y_pred  = (y_pred == 'Sim').astype(int)\n",
    "\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "    print(f\"Test: Accuracy: {str(accuracy).replace('.',',')} | Precision:{str(precision).replace('.',',')} | Recall:{str(recall).replace('.',',')} | F1-score:{str(f1).replace('.',',')}\")\n",
    "    print(f\"{str(accuracy).replace('.',',')[1:-2]}\\t{str(precision).replace('.',',')}\\t{str(recall).replace('.',',')}\\t{str(f1).replace('.',',')}\")\n",
    "\n",
    "y_pred = pipeline.predict(X_test.copy())\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "y_true  = (y_test == 'Sim').astype(int)\n",
    "y_pred  = (y_pred == 'Sim').astype(int)\n",
    "\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "FP = np.sum((y_pred == 1) & (y_true == 0))\n",
    "VP = np.sum((y_pred == 1) & (y_true == 1))\n",
    "FN = np.sum((y_pred == 0) & (y_true == 1))\n",
    "VN = np.sum((y_pred == 0) & (y_true == 0))\n",
    "\n",
    "print(f\"VP: {VP} - VN: {VN} - FP: {FP} - FN: {FN}\")\n",
    "print(f\"{VP}\\t{VN}\\t{FP}\\t{FN}\")\n",
    "print(f\"Test: Accuracy: {str(accuracy).replace('.',',')} | Precision:{str(precision).replace('.',',')} | Recall:{str(recall).replace('.',',')} | F1-score:{str(f1).replace('.',',')}\")\n",
    "print(f\"{str(accuracy).replace('.',',')[1:-2]}\\t{str(precision).replace('.',',')}\\t{str(recall).replace('.',',')}\\t{str(f1).replace('.',',')}\")\n",
    "\n",
    "class_labels = pipeline.classes_\n",
    "class_index = np.where(class_labels == \"Sim\")[0][0]\n",
    "\n",
    "y_probs = pipeline.predict_proba(X_test.copy())[:, class_index]\n",
    "\n",
    "y_true  = (y_test.copy() == 'Sim').astype(int)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_true, y_probs)\n",
    "pr_auc = auc(recall, precision)\n",
    "print('auc: ', pr_auc)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, marker='.')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precisão')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "\n",
    "print(data.shape[0])\n",
    "data = data.dropna(subset=['Vai Chover Amanha'])#Removendo linhas com valores faltantes na coluna de outliers\n",
    "\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "print(data.shape[0])\n",
    "\n",
    "X = data.drop('Vai Chover Amanha', axis=1)\n",
    "y = data['Vai Chover Amanha']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)\n",
    "\n",
    "    \n",
    "\n",
    "def objective(trial):\n",
    "    loss = trial.suggest_categorical('loss', ['hinge', 'modified_huber', 'perceptron', 'squared_hinge', 'squared_error',\n",
    "                                                    'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'])\n",
    "    penalty = trial.suggest_categorical('penalty', ['l2', 'l1', 'elasticnet'])\n",
    "    alpha = trial.suggest_float('alpha', 1e-5, 1e-3)\n",
    "    tol = trial.suggest_float('tol', 1e-4, 1e-2)\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('manual',ManualFeatureSelectorTransformer()),\n",
    "        ('imputer',ImputerTransformer()),\n",
    "        ('date', DateTransformer()),\n",
    "        ('scaler', RobustScalerTransformer()),\n",
    "        ('os', RandomOverSampler(random_state=42)),\n",
    "        ('model', SGDClassifier(max_iter=4000, tol=tol, alpha=alpha, penalty=penalty, loss=loss, class_weight='balanced', random_state=42)),\n",
    "        ])\n",
    "\n",
    "    X_train_2, X_val, y_train_2, y_val = train_test_split(X_train.copy(), y_train.copy(), stratify=y_train, test_size=0.177, random_state=42)\n",
    "    pipeline.fit(X_train_2, y_train_2)\n",
    "\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "    y_true  = (y_val == 'Sim').astype(int)\n",
    "    y_pred  = (y_pred == 'Sim').astype(int)\n",
    "\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"Accuracy: {str(accuracy).replace('.',',')} | Precision:{str(precision).replace('.',',')} | Recall:{str(recall).replace('.',',')} | F1-score:{str(f1).replace('.',',')}\")\n",
    "    #score = cross_val_score(pipeline, X.copy(), y.copy(), n_jobs=-1, cv=5).mean()\n",
    "    return f1\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "for trial in study.trials:\n",
    "    print(f\"Trial {trial.number}:\")\n",
    "    print(f\"  Value: {trial.value}\")\n",
    "    print(\"  Params:\")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n\\n\\nBest trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(f\" Val Value: {trial.value}\")\n",
    "print(\" Val Params:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('manual',ManualFeatureSelectorTransformer()),\n",
    "    ('imputer',ImputerTransformer()),\n",
    "    ('date', DateTransformer()),\n",
    "    ('scaler', RobustScalerTransformer()),\n",
    "    ('os', RandomOverSampler(random_state=42)),\n",
    "    ('model', SGDClassifier(max_iter=4000, tol=trial.params['tol'], alpha=trial.params['alpha'], penalty=trial.params['penalty'], loss=trial.params['loss'], class_weight='balanced', random_state=42)),\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "y_true  = (y_test == 'Sim').astype(int)\n",
    "y_pred  = (y_pred == 'Sim').astype(int)\n",
    "\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Test: Accuracy: {str(accuracy).replace('.',',')} | Precision:{str(precision).replace('.',',')} | Recall:{str(recall).replace('.',',')} | F1-score:{str(f1).replace('.',',')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "data = data.dropna(subset=['Vai Chover Amanha'])#Removendo linhas com valores faltantes na coluna de outliers\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X = data.drop('Vai Chover Amanha', axis=1)\n",
    "y = data['Vai Chover Amanha']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('manual',ManualFeatureSelectorTransformer()),\n",
    "    ('imputer',ImputerTransformer()),\n",
    "    ('date', DateTransformer()),\n",
    "    ('scaler', RobustScalerTransformer()),\n",
    "    ('os', RandomOverSampler(random_state=42)),\n",
    "    ('model', SGDClassifier(max_iter=4000, tol=0.002461489467454204, alpha=0.0005825682471131291, penalty='l1', loss=\"modified_huber\", class_weight='balanced', random_state=42)),\n",
    "    ])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "for i in range(2001,2025):\n",
    "    print(i)\n",
    "    f_X_test = X_test.copy().loc[pd.to_datetime(X_test['Data']).dt.year == i]\n",
    "    f_X_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    f_y_test = y_test.copy().loc[pd.to_datetime(X_test['Data']).dt.year == i]\n",
    "    f_y_test.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "\n",
    "    y_pred = pipeline.predict(f_X_test)\n",
    "    accuracy = accuracy_score(f_y_test, y_pred)\n",
    "\n",
    "    y_true  = (f_y_test == 'Sim').astype(int)\n",
    "    y_pred  = (y_pred == 'Sim').astype(int)\n",
    "\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "    print(f\"Test: Accuracy: {str(accuracy).replace('.',',')} | Precision:{str(precision).replace('.',',')} | Recall:{str(recall).replace('.',',')} | F1-score:{str(f1).replace('.',',')}\")\n",
    "    print(f\"{str(accuracy).replace('.',',')[1:-2]}\\t{str(precision).replace('.',',')}\\t{str(recall).replace('.',',')}\\t{str(f1).replace('.',',')}\")\n",
    "\n",
    "for cidade in data['Cidade'].unique():\n",
    "    print(cidade)\n",
    "    f_X_test = X_test.copy().loc[X_test['Cidade'] == cidade]\n",
    "    f_X_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    f_y_test = y_test.copy().loc[X_test['Cidade'] == cidade]\n",
    "    f_y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    y_pred = pipeline.predict(f_X_test)\n",
    "    accuracy = accuracy_score(f_y_test, y_pred)\n",
    "\n",
    "    y_true  = (f_y_test == 'Sim').astype(int)\n",
    "    y_pred  = (y_pred == 'Sim').astype(int)\n",
    "\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "    print(f\"Test: Accuracy: {str(accuracy).replace('.',',')} | Precision:{str(precision).replace('.',',')} | Recall:{str(recall).replace('.',',')} | F1-score:{str(f1).replace('.',',')}\")\n",
    "    print(f\"{str(accuracy).replace('.',',')[1:-2]}\\t{str(precision).replace('.',',')}\\t{str(recall).replace('.',',')}\\t{str(f1).replace('.',',')}\")\n",
    "\n",
    "y_pred = pipeline.predict(X_test.copy())\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "y_true  = (y_test == 'Sim').astype(int)\n",
    "y_pred  = (y_pred == 'Sim').astype(int)\n",
    "\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "FP = np.sum((y_pred == 1) & (y_true == 0))\n",
    "VP = np.sum((y_pred == 1) & (y_true == 1))\n",
    "FN = np.sum((y_pred == 0) & (y_true == 1))\n",
    "VN = np.sum((y_pred == 0) & (y_true == 0))\n",
    "\n",
    "print(f\"VP: {VP} - VN: {VN} - FP: {FP} - FN: {FN}\")\n",
    "print(f\"{VP}\\t{VN}\\t{FP}\\t{FN}\")\n",
    "print(f\"Test: Accuracy: {str(accuracy).replace('.',',')} | Precision:{str(precision).replace('.',',')} | Recall:{str(recall).replace('.',',')} | F1-score:{str(f1).replace('.',',')}\")\n",
    "print(f\"{str(accuracy).replace('.',',')[1:-2]}\\t{str(precision).replace('.',',')}\\t{str(recall).replace('.',',')}\\t{str(f1).replace('.',',')}\")\n",
    "\n",
    "class_labels = pipeline.classes_\n",
    "class_index = np.where(class_labels == \"Sim\")[0][0]\n",
    "\n",
    "y_probs = pipeline.predict_proba(X_test.copy())[:, class_index]\n",
    "\n",
    "y_true  = (y_test.copy() == 'Sim').astype(int)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_true, y_probs)\n",
    "pr_auc = auc(recall, precision)\n",
    "print('auc: ', pr_auc)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, marker='.')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precisão')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variancia explicada cumulativa\n",
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "\n",
    "data = data.dropna(subset=['Vai Chover Amanha'])\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X = data.drop('Vai Chover Amanha', axis=1)\n",
    "y = data['Vai Chover Amanha']\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('manual',ManualFeatureSelectorTransformer()),\n",
    "    ('imputer',ImputerTransformer()),\n",
    "    ('date', DateTransformer()),\n",
    "    ('scaler', RobustScalerTransformer()),\n",
    "])\n",
    "\n",
    "X_train = pipeline.fit_transform(X, y)\n",
    "\n",
    "nums = np.arange(33)\n",
    "var_ratio = []\n",
    "for num in nums:\n",
    "  pca = PCA(n_components=num)\n",
    "  pca.fit(X_train)\n",
    "  var_ratio.append(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "print(nums)\n",
    "for num in nums:\n",
    "  print(num, str(var_ratio[num]).replace('.',','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#descrição do conjunto de dados após a normalização\n",
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "\n",
    "data = data.dropna(subset=['Vai Chover Amanha'])\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X = data.drop('Vai Chover Amanha', axis=1)\n",
    "y = data['Vai Chover Amanha']\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('manual',ManualFeatureSelectorTransformer()),\n",
    "    ('imputer',ImputerTransformer()),\n",
    "    ('date', DateTransformer()),\n",
    "    ('scaler', RobustScalerTransformer()),\n",
    "])\n",
    "\n",
    "X_train = pipeline.fit_transform(X, y)\n",
    "print(X_train[['Precipitacao Total', 'Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima',\n",
    "                'Umidade Minima', 'Umidade Maxima','Precipitacao Total', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media']].describe())\n",
    "\n",
    "print(X_train[['Umidade Media', 'Direcao Vento 0H','Direcao Vento 6H','Direcao Vento 12H','Direcao Vento 18H', 'Rajada Maxima de Vento 0H', 'Rajada Maxima de Vento 6H',\n",
    "                'Rajada Maxima de Vento 12H', 'Rajada Maxima de Vento 18H']].describe())\n",
    "\n",
    "print(X_train[['Vento Velocidade Media 0H', 'Vento Velocidade Media 6H', 'Vento Velocidade Media 12H', 'Vento Velocidade Media 18H', 'Radiacao Global', 'Altitude']].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analise dos valores faltantes\n",
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "\n",
    "data.drop('Radiacao Global', axis=1, inplace=True)\n",
    "data.drop('Cidade', axis=1, inplace=True)\n",
    "data.drop('Codigo', axis=1, inplace=True)\n",
    "data.drop('Latitude', axis=1, inplace=True)\n",
    "data.drop('Longitude', axis=1, inplace=True)\n",
    "\n",
    "data['Data'] = pd.to_datetime(data['Data'])\n",
    "data['Ano'] = data['Data'].dt.year\n",
    "data.drop('Data', axis=1, inplace=True)\n",
    "\n",
    "print('any:')\n",
    "counter = 0\n",
    "for i in range(2001,2025):\n",
    "    filtro = data.loc[(data['Ano']  == i)].drop('Ano', axis=1)\n",
    "    counter += filtro.isna().any(axis=1).sum()\n",
    "    print(f'{i}\\t{filtro.isna().any(axis=1).sum()}')\n",
    "print(f'Total\\t{counter}')\n",
    "\n",
    "print('all:')\n",
    "counter = 0\n",
    "for i in range(2001,2025):\n",
    "    filtro = data.loc[(data['Ano']  == i)].drop('Ano', axis=1)\n",
    "    counter += filtro.isna().all(axis=1).sum()\n",
    "    print(f'{i}\\t{filtro.isna().all(axis=1).sum()}')\n",
    "print(f'Total\\t{counter}')\n",
    "\n",
    "print('i:')\n",
    "counter = 0\n",
    "for i in range(2001,2025):\n",
    "    counter += data.loc[(data['Ano']  == i)]['Vai Chover Amanha'].isna().sum()\n",
    "    value = data.loc[(data['Ano']  == i)]['Vai Chover Amanha'].isna().sum()\n",
    "    print(f'{i}\\t{value}')\n",
    "print(f'Total\\t{counter}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analise dos outliers usando o z-score\n",
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "\n",
    "data = data.dropna()\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "columns = ['Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima',\n",
    "                'Umidade Minima', 'Umidade Maxima','Precipitacao Total', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media', 'Umidade Media', \n",
    "                'Direcao Vento 0H','Direcao Vento 6H','Direcao Vento 12H','Direcao Vento 18H', 'Rajada Maxima de Vento 0H', 'Rajada Maxima de Vento 6H',\n",
    "                'Rajada Maxima de Vento 12H', 'Rajada Maxima de Vento 18H', 'Vento Velocidade Media 0H', 'Vento Velocidade Media 6H',\n",
    "                'Vento Velocidade Media 12H', 'Vento Velocidade Media 18H', 'Radiacao Global', 'Altitude']\n",
    "\n",
    "counter = 0\n",
    "for column in columns:\n",
    "    z_scores = np.abs(stats.zscore(data[column]))\n",
    "    threshold = 3\n",
    "    outliers_mask = (z_scores > threshold)\n",
    "\n",
    "    num_outliers = outliers_mask.sum()\n",
    "    print(f\"{column}\\t{num_outliers}\")\n",
    "    counter += outliers_mask.sum()\n",
    "print(counter)\n",
    "\n",
    "column = 'Precipitacao Total'\n",
    "data = data[data[column] != 0]\n",
    "\n",
    "z_scores = np.abs(stats.zscore(data[column]))\n",
    "threshold = 3\n",
    "outliers_mask = (z_scores > threshold)\n",
    "\n",
    "num_outliers = outliers_mask.sum()\n",
    "print(f\"{column}\\t{num_outliers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#descrição geral dos dados\n",
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "\n",
    "data = data.dropna()\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(data[['Precipitacao Total', 'Pressao Maxima','Pressao Minima','Temperatura Maxima','Temperatura Minima','Temperatura Orvalho Maxima','Temperatura Orvalho Minima',\n",
    "                'Umidade Minima', 'Umidade Maxima','Precipitacao Total', 'Pressao Media', 'Temperatura Media', 'Temperatura Orvalho Media']].describe())\n",
    "\n",
    "print(data[['Umidade Media', 'Direcao Vento 0H','Direcao Vento 6H','Direcao Vento 12H','Direcao Vento 18H', 'Rajada Maxima de Vento 0H', 'Rajada Maxima de Vento 6H',\n",
    "                'Rajada Maxima de Vento 12H', 'Rajada Maxima de Vento 18H']].describe())\n",
    "\n",
    "print(data[['Vento Velocidade Media 0H', 'Vento Velocidade Media 6H', 'Vento Velocidade Media 12H', 'Vento Velocidade Media 18H', 'Radiacao Global', 'Altitude']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contagem de valores por classe no atributo alvo\n",
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "\n",
    "print(data['Vai Chover Amanha'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "\n",
    "#busca por outliers nos atributos de latitude e longitude\n",
    "for value in data.dropna()['Codigo'].unique():\n",
    "    filtered = data[(data['Codigo'] == value)]\n",
    "    lat_mode = filtered['Latitude'].mode()[0]\n",
    "    long_mode = filtered['Longitude'].mode()[0]\n",
    "    alt_mode = filtered['Altitude'].mode()[0]\n",
    "    lat_outliers = (filtered['Latitude'] != lat_mode).sum()\n",
    "    long_outliers = (filtered['Longitude'] != long_mode).sum()\n",
    "    alt_outliers = (filtered['Altitude'] != alt_mode).sum()\n",
    "    print(\"cod - \", value, \" lat: \", lat_mode, \" outliers: \", lat_outliers, \" - long: \", long_mode, \" outliers: \", long_outliers, \" - alt: \", alt_mode, \" outliers: \", alt_outliers)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
